{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=512, fc2_units=512):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Policy, self).__init__()\n",
    "        #self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "        self.std = nn.Parameter(torch.zeros(action_size))\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action, action distribution (for continuous spaces).\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "        \n",
    "        \n",
    "class Value(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, fc1_units=512, fc2_units=512):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Value, self).__init__()\n",
    "        #self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> value.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class A2C(nn.Module):\n",
    "    \"\"\"A2C Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(A2C, self).__init__()\n",
    "        self.actor = Policy(state_size, action_size)\n",
    "        self.critic = Value(state_size)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.zeros(1, action_size))\n",
    "\n",
    "    def forward(self, state, action=None):\n",
    "        \"\"\"Build a network that maps state -> action, action distribution (for continuous spaces).\"\"\"\n",
    "        mean = self.actor(state)\n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(-1).unsqueeze(-1)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        value = self.critic(state)\n",
    "        return action, log_prob, entropy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99            # discount factor\n",
    "TAU = .95\n",
    "ACTOR_LR = 3e-4         # actor learning rate\n",
    "OPTIMIZATION_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "PPO_CLIP = .2\n",
    "GRADIENT_CLIP = 5\n",
    "EPS = 1e-5\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device('cpu')\n",
    "print (device)\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        #self.seed = random.seed(seed)\n",
    "\n",
    "        # Networks\n",
    "        #self.actor_network = Policy(state_size, action_size, seed).to(device)\n",
    "        #self.critic_network = Value(state_size, seed).to(device)\n",
    "        #self.actor_optimizer = optim.Adam(self.actor_network.parameters(), lr=ACTOR_LR)\n",
    "        #self.critic_optimizer = optim.Adam(self.critic_network.parameters(), lr=CRITIC_LR)\n",
    "        \n",
    "        self.network = A2C(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=ACTOR_LR, eps=EPS)\n",
    "        \n",
    "    def step(self, roll_log_probs, roll_entropies, roll_values, roll_rewards, roll_dones, roll_actions, roll_states, next_values):\n",
    "        \"\"\"Updates actor network with loss of: advantage * log_probability + entropy\n",
    "           Updates critic network with loss of: (TD Error)^2\n",
    "           \n",
    "        Params\n",
    "        ======\n",
    "            states (array_like): current states of all parallel agents\n",
    "            next_states (array_like): next states of all parallel agents\n",
    "            actions (array_like): chosen actions of all parallel agents\n",
    "            dists (Torch object): action distributions of all chosen actions\n",
    "            rewards (array_like): rewards observed for all parallel agents\n",
    "            dones (array_like): booleans (True if terminal state, False otherwise)\n",
    "        \"\"\"        \n",
    "        # Initialize tensors\n",
    "        #print (states)\n",
    "        #states = torch.from_numpy(states).float().to(device)\n",
    "        #actions = torch.from_numpy(actions).float().to(device)\n",
    "        #dists = torch.tensor(states).float().unsqueeze(1).to(device)\n",
    "        #rewards = torch.from_numpy(np.vstack([r for r in rewards])).float().to(device)\n",
    "        #next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        #dones = torch.from_numpy(np.vstack([d for d in dones]).astype(np.uint8)).float().to(device)\n",
    "                \n",
    "        # Obtain state values from critic network      \n",
    "        #next_state_values = self.network(next_states)[3]\n",
    "        #print (next_state_values)\n",
    "        #state_values = self.critic_network(states)\n",
    "        #state_values = values\n",
    "        #print (state_values)\n",
    "        \n",
    "        next_td = next_values\n",
    "        next_state_values = next_values\n",
    "        advantages = torch.tensor([]).float().to(device)\n",
    "        #last_advantages = torch.from_numpy(np.vstack([0 for _ in range(20)])).float().to(device)\n",
    "        last_advantages = torch.tensor(0).float().to(device)\n",
    "        log_probs_old = torch.tensor([]).float().to(device)\n",
    "        entropies = torch.tensor([]).float().to(device)\n",
    "        values = torch.tensor([]).float().to(device)\n",
    "        td_estimates = torch.tensor([]).float().to(device)\n",
    "        states = torch.tensor([]).float().to(device)\n",
    "        actions = torch.tensor([]).float().to(device)\n",
    "        for i in reversed(range(len(roll_log_probs))):\n",
    "            # Initialize tensors\n",
    "            rewards = torch.from_numpy(np.vstack([r for r in roll_rewards[i]])).float().to(device)\n",
    "            #state_values = roll_values[i].detach()\n",
    "            state_values = roll_values[i]\n",
    "            #print (state_values.requires_grad)\n",
    "            dones = torch.from_numpy(np.vstack([d for d in roll_dones[i]]).astype(np.uint8)).float().to(device)\n",
    "            i_log_probs = roll_log_probs[i]\n",
    "            i_entropies = roll_entropies[i]\n",
    "            i_actions = roll_actions[i]\n",
    "            i_states = torch.from_numpy(np.vstack([s for s in roll_states[i]])).float().to(device)\n",
    "            #i_states = torch.from_numpy(roll_states[i]).float().to(device)\n",
    "            \n",
    "            # Compute TD Estimates and action advantages\n",
    "            print ('Next TD:\\t', next_td.size(0), ' * ', next_td.size(1))\n",
    "            print ('rewards:\\t', rewards.size(0), ' * ', rewards.size(1))\n",
    "            print ('Dones:\\t\\t', dones.size(0), ' * ', dones.size(1))\n",
    "            next_td = rewards + (GAMMA * next_td * (1 - dones))\n",
    "            print ()\n",
    "            print ('rewards:\\t', rewards.size(0), ' * ', rewards.size(1))\n",
    "            print ('NS Values:\\t', next_state_values.size(0), ' * ', next_state_values.size(1))\n",
    "            print ('S Values:\\t', state_values.size(0), ' * ', state_values.size(1))\n",
    "            print ('Dones:\\t\\t', dones.size(0), ' * ', dones.size(1))\n",
    "            td_error = rewards + (GAMMA * next_state_values * (1 - dones)) - state_values\n",
    "            print ()\n",
    "            print ('Last A:\\t', last_advantages)\n",
    "            print ('Dones:\\t\\t', dones.size(0), ' * ', dones.size(1))\n",
    "            print ('TD Error:\\t', td_error.size(0), ' * ', td_error.size(1))\n",
    "            i_advantages = last_advantages * TAU * GAMMA * (1 - dones) + td_error\n",
    "            last_advantages = i_advantages\n",
    "            \n",
    "            # Concatenate tensors\n",
    "            advantages = torch.cat((advantages, i_advantages.detach()), 0)\n",
    "            log_probs_old = torch.cat((log_probs_old, i_log_probs.detach()), 0)\n",
    "            entropies = torch.cat((entropies, i_entropies), 0)\n",
    "            values = torch.cat((values, roll_values[i]), 0)\n",
    "            td_estimates = torch.cat((td_estimates, next_td), 0)\n",
    "            actions = torch.cat((actions, i_actions), 0)\n",
    "            states = torch.cat((states, i_states))\n",
    "           \n",
    "        #print ('States: ', states.size(0), ' * ', states.size(1))\n",
    "        #print ('Advantages: ', advantages.size(0), ' * ', advantages.size(1))\n",
    "        advantage = (advantages - advantages.mean()) / advantages.std()\n",
    "        #print ('Advantage: ', advantage.size(0), ' * ', advantage.size(1))\n",
    "        \n",
    "        #print ('State requires_grad: ', states.requires_grad)\n",
    "        #print ('Action requires_grad: ', actions.requires_grad)\n",
    "        #print ('Log Prob requires_grad: ', log_probs_old.requires_grad)\n",
    "        #print ('TD requires_grad: ', td_estimates.requires_grad)\n",
    "        #print ('Advantage requires_grad: ', advantages.requires_grad)\n",
    "        self.learn(states, actions, log_probs_old, td_estimates, advantages)\n",
    "        \n",
    "        #print (advantages.requires_grad)\n",
    "        # Compute policy and value losses\n",
    "        #policy_loss = -(log_probs * advantages).mean()\n",
    "        #value_loss = .5 * (td_estimates - values).pow(2).mean()\n",
    "        #print ('Policy loss: {0}\\tValue loss: {1}'.format(policy_loss, value_loss))\n",
    "        #total_loss = policy_loss - .01 * entropies.mean() + value_loss\n",
    "        #print (total_loss)\n",
    "        \n",
    "        # Optimize actor and critic parameters\n",
    "        #self.actor_optimizer.zero_grad()\n",
    "        #policy_loss.backward(retain_graph=True)\n",
    "        #self.actor_optimizer.step()\n",
    "        \n",
    "        #self.critic_optimizer.zero_grad()\n",
    "        #value_loss.backward()\n",
    "        #self.critic_optimizer.step()\n",
    "        \n",
    "    def learn (self, states, actions, log_probs_old, td_estimates, advantages):\n",
    "        for _ in range(OPTIMIZATION_EPOCHS):\n",
    "            for _ in range(states.size(0) // BATCH_SIZE):\n",
    "                sample_indices = torch.from_numpy(np.random.choice(states.size(0), BATCH_SIZE)).long().to(device)\n",
    "                #print ('Index: ', sample_indices)\n",
    "                sampled_states = states[sample_indices, :]\n",
    "                #print ('Sampled States: ', sampled_states.size(0), ' * ', sampled_states.size(1))\n",
    "                #print (sampled_states)\n",
    "                sampled_actions = actions[sample_indices, :]\n",
    "                #print ('Sampled Actions: ', sampled_actions.size(0), ' * ', sampled_actions.size(1))\n",
    "                sampled_log_probs_old = log_probs_old[sample_indices, :]\n",
    "                #print ('Sampled Log Probs: ', sampled_log_probs_old.size(0), ' * ', sampled_log_probs_old.size(1))\n",
    "                sampled_td_estimates = td_estimates[sample_indices, :]\n",
    "                #print ('Sampled TDs: ', sampled_td_estimates.size(0), ' * ', sampled_td_estimates.size(1))\n",
    "                sampled_advantages = advantages[sample_indices, :]\n",
    "                #print ('Sampled Advantages: ', sampled_advantages.size(0), ' * ', sampled_advantages.size(1))\n",
    "            \n",
    "                p_actions, p_log_probs, p_entropies, p_values = agent.network(sampled_states, sampled_actions)\n",
    "                #print ('Entropy:\\t', p_entropies.mean())\n",
    "                ratio = (p_log_probs - sampled_log_probs_old).exp()\n",
    "                sur = ratio * sampled_advantages\n",
    "                #print ('Sur:\\t', sur)\n",
    "                clipped_sur = ratio.clamp(1.0 - PPO_CLIP, 1.0 + PPO_CLIP) * sampled_advantages\n",
    "                #print ('Clipped:\\t', clipped_sur)\n",
    "                #print ('Clipped Sur: ', clipped_sur.size(0), ' * ', clipped_sur.size(1))\n",
    "                policy_loss = -torch.min(sur, clipped_sur).mean(0) - .01 * p_entropies.mean()\n",
    "                #print ('Policy Loss:\\t', policy_loss)\n",
    "                value_loss = (sampled_td_estimates - p_values).pow(2).mean()\n",
    "                #print ('Value Loss:\\t', value_loss)\n",
    "                #total_loss = policy_loss + value_loss\n",
    "            \n",
    "                self.optimizer.zero_grad()\n",
    "                (policy_loss + .5 * value_loss).backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), GRADIENT_CLIP)\n",
    "                self.optimizer.step()\n",
    "        \n",
    "    def act(self, states, train=False):\n",
    "        \"\"\"Returns action and action distribution for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "        \"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        #self.actor_network.eval()\n",
    "        if train:\n",
    "            actions, log_probs, entropies, values = self.network(states)\n",
    "        else:\n",
    "            self.network.eval()\n",
    "            with torch.no_grad():\n",
    "                actions, log_probs, entropies, values = self.network(states)\n",
    "            #self.actor_network.train()\n",
    "            self.network.train()\n",
    "        #print (actions)\n",
    "        actions = actions.squeeze(1)\n",
    "        #print (actions)                \n",
    "        #dists = dists.cpu().data.numpy()\n",
    "        #print (dists)\n",
    "        return actions, log_probs, entropies, values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher20_Windows_x86_64\\\\Reacher.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor(0., device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.5472],\n",
      "        [-0.0689],\n",
      "        [-0.1649],\n",
      "        [ 0.0907],\n",
      "        [ 0.0814],\n",
      "        [-0.0353],\n",
      "        [ 0.0829],\n",
      "        [ 0.0157],\n",
      "        [ 0.2960],\n",
      "        [ 0.4945],\n",
      "        [-0.2064],\n",
      "        [ 0.0476],\n",
      "        [ 0.1430],\n",
      "        [ 0.0187],\n",
      "        [ 0.3358],\n",
      "        [ 0.1751],\n",
      "        [ 0.0077],\n",
      "        [-0.0786],\n",
      "        [-0.0690],\n",
      "        [ 0.0030]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.0966],\n",
      "        [ 0.1135],\n",
      "        [-0.4774],\n",
      "        [-0.1416],\n",
      "        [-0.0116],\n",
      "        [ 0.0861],\n",
      "        [-0.0705],\n",
      "        [-0.0849],\n",
      "        [ 0.4721],\n",
      "        [ 0.7271],\n",
      "        [-0.4933],\n",
      "        [ 0.0907],\n",
      "        [ 0.2712],\n",
      "        [-0.0215],\n",
      "        [ 0.3195],\n",
      "        [ 0.1940],\n",
      "        [-0.1769],\n",
      "        [-0.4121],\n",
      "        [-0.4174],\n",
      "        [-0.0873]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.3203],\n",
      "        [ 0.3889],\n",
      "        [-0.8485],\n",
      "        [-0.2978],\n",
      "        [-0.0803],\n",
      "        [ 0.1416],\n",
      "        [-0.2089],\n",
      "        [-0.1894],\n",
      "        [ 0.3713],\n",
      "        [ 0.8381],\n",
      "        [-0.7494],\n",
      "        [ 0.2208],\n",
      "        [ 0.4012],\n",
      "        [-0.0030],\n",
      "        [ 0.3835],\n",
      "        [ 0.3209],\n",
      "        [-0.4233],\n",
      "        [-0.7541],\n",
      "        [-0.7473],\n",
      "        [-0.1547]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.2700],\n",
      "        [ 0.6752],\n",
      "        [-1.2250],\n",
      "        [-0.4245],\n",
      "        [-0.1019],\n",
      "        [-0.0509],\n",
      "        [-0.3425],\n",
      "        [-0.2795],\n",
      "        [ 0.2679],\n",
      "        [ 0.9139],\n",
      "        [-0.9799],\n",
      "        [ 0.3993],\n",
      "        [ 0.5254],\n",
      "        [-0.1080],\n",
      "        [ 0.4174],\n",
      "        [ 0.3223],\n",
      "        [-0.6005],\n",
      "        [-1.1067],\n",
      "        [-0.8708],\n",
      "        [-0.1925]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.2070],\n",
      "        [ 0.8859],\n",
      "        [-1.4427],\n",
      "        [-0.4952],\n",
      "        [-0.0226],\n",
      "        [-0.1921],\n",
      "        [-0.4322],\n",
      "        [-0.3776],\n",
      "        [ 0.2168],\n",
      "        [ 0.9417],\n",
      "        [-1.1215],\n",
      "        [ 0.5100],\n",
      "        [ 0.6392],\n",
      "        [-0.1664],\n",
      "        [ 0.5529],\n",
      "        [ 0.3960],\n",
      "        [-0.8230],\n",
      "        [-1.4624],\n",
      "        [-0.7633],\n",
      "        [-0.1392]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.0630],\n",
      "        [ 1.0993],\n",
      "        [-1.6470],\n",
      "        [-0.4926],\n",
      "        [ 0.1799],\n",
      "        [-0.2489],\n",
      "        [-0.5263],\n",
      "        [-0.4379],\n",
      "        [ 0.2617],\n",
      "        [ 0.9528],\n",
      "        [-1.1851],\n",
      "        [ 0.4669],\n",
      "        [ 0.7308],\n",
      "        [-0.1687],\n",
      "        [ 0.4983],\n",
      "        [ 0.5232],\n",
      "        [-1.0030],\n",
      "        [-1.8048],\n",
      "        [-0.6865],\n",
      "        [-0.0607]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.8724],\n",
      "        [ 1.2416],\n",
      "        [-1.8332],\n",
      "        [-0.4811],\n",
      "        [ 0.4496],\n",
      "        [-0.2363],\n",
      "        [-0.6141],\n",
      "        [-0.5079],\n",
      "        [ 0.4124],\n",
      "        [ 0.9578],\n",
      "        [-1.2478],\n",
      "        [ 0.3946],\n",
      "        [ 0.8128],\n",
      "        [-0.2731],\n",
      "        [ 0.3694],\n",
      "        [ 0.6062],\n",
      "        [-1.1558],\n",
      "        [-2.0786],\n",
      "        [-0.6114],\n",
      "        [ 0.1014]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.6559],\n",
      "        [ 1.3869],\n",
      "        [-1.9520],\n",
      "        [-0.5863],\n",
      "        [ 0.5862],\n",
      "        [-0.1940],\n",
      "        [-0.6921],\n",
      "        [-0.5654],\n",
      "        [ 0.5689],\n",
      "        [ 0.9620],\n",
      "        [-1.3268],\n",
      "        [ 0.3234],\n",
      "        [ 0.8710],\n",
      "        [-0.4146],\n",
      "        [ 0.3206],\n",
      "        [ 0.7100],\n",
      "        [-1.3252],\n",
      "        [-2.2955],\n",
      "        [-0.5362],\n",
      "        [ 0.2244]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.4687],\n",
      "        [ 1.5056],\n",
      "        [-2.0275],\n",
      "        [-0.7292],\n",
      "        [ 0.7864],\n",
      "        [-0.1348],\n",
      "        [-0.7297],\n",
      "        [-0.6253],\n",
      "        [ 0.7641],\n",
      "        [ 0.9320],\n",
      "        [-1.4623],\n",
      "        [ 0.2144],\n",
      "        [ 0.9033],\n",
      "        [-0.5380],\n",
      "        [ 0.2549],\n",
      "        [ 0.8429],\n",
      "        [-1.4605],\n",
      "        [-2.4339],\n",
      "        [-0.5463],\n",
      "        [ 0.3264]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.2415],\n",
      "        [ 1.6095],\n",
      "        [-2.2160],\n",
      "        [-0.8593],\n",
      "        [ 0.8909],\n",
      "        [-0.0259],\n",
      "        [-0.7545],\n",
      "        [-0.6687],\n",
      "        [ 0.9579],\n",
      "        [ 0.9071],\n",
      "        [-1.6280],\n",
      "        [ 0.0557],\n",
      "        [ 0.9245],\n",
      "        [-0.6589],\n",
      "        [ 0.1433],\n",
      "        [ 0.8819],\n",
      "        [-1.5891],\n",
      "        [-2.6526],\n",
      "        [-0.6283],\n",
      "        [ 0.4294]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.0199],\n",
      "        [ 1.6828],\n",
      "        [-2.4149],\n",
      "        [-1.0376],\n",
      "        [ 1.0143],\n",
      "        [ 0.0381],\n",
      "        [-0.8099],\n",
      "        [-0.7417],\n",
      "        [ 1.1580],\n",
      "        [ 0.8912],\n",
      "        [-1.7724],\n",
      "        [-0.1484],\n",
      "        [ 0.9720],\n",
      "        [-0.8149],\n",
      "        [-0.0180],\n",
      "        [ 0.8976],\n",
      "        [-1.7580],\n",
      "        [-2.9482],\n",
      "        [-0.7808],\n",
      "        [ 0.3948]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.1880],\n",
      "        [ 1.6854],\n",
      "        [-2.6435],\n",
      "        [-1.2358],\n",
      "        [ 1.0294],\n",
      "        [ 0.0618],\n",
      "        [-0.8906],\n",
      "        [-0.8596],\n",
      "        [ 1.3191],\n",
      "        [ 0.9544],\n",
      "        [-1.9459],\n",
      "        [-0.3172],\n",
      "        [ 1.0120],\n",
      "        [-1.0215],\n",
      "        [-0.1503],\n",
      "        [ 0.8468],\n",
      "        [-1.8772],\n",
      "        [-3.0878],\n",
      "        [-1.0241],\n",
      "        [ 0.4487]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.3649],\n",
      "        [ 1.6638],\n",
      "        [-2.8938],\n",
      "        [-1.3785],\n",
      "        [ 0.9937],\n",
      "        [ 0.0555],\n",
      "        [-0.9920],\n",
      "        [-0.9906],\n",
      "        [ 1.2512],\n",
      "        [ 0.9475],\n",
      "        [-2.0971],\n",
      "        [-0.5339],\n",
      "        [ 0.9682],\n",
      "        [-1.2461],\n",
      "        [-0.2831],\n",
      "        [ 0.8809],\n",
      "        [-1.9032],\n",
      "        [-3.1268],\n",
      "        [-1.2940],\n",
      "        [ 0.5744]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.5939],\n",
      "        [ 1.6343],\n",
      "        [-2.9913],\n",
      "        [-1.4790],\n",
      "        [ 0.9199],\n",
      "        [ 0.0450],\n",
      "        [-1.1098],\n",
      "        [-1.0722],\n",
      "        [ 1.4551],\n",
      "        [ 1.0068],\n",
      "        [-2.3171],\n",
      "        [-0.6527],\n",
      "        [ 0.9155],\n",
      "        [-1.5111],\n",
      "        [-0.3852],\n",
      "        [ 0.8990],\n",
      "        [-1.8673],\n",
      "        [-3.0169],\n",
      "        [-1.5708],\n",
      "        [ 0.6984]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.8033],\n",
      "        [ 1.5345],\n",
      "        [-3.0778],\n",
      "        [-1.5709],\n",
      "        [ 0.8494],\n",
      "        [ 0.0328],\n",
      "        [-1.2607],\n",
      "        [-1.1689],\n",
      "        [ 1.5813],\n",
      "        [ 1.0446],\n",
      "        [-2.6431],\n",
      "        [-0.7514],\n",
      "        [ 0.7942],\n",
      "        [-1.7927],\n",
      "        [-0.4378],\n",
      "        [ 0.8281],\n",
      "        [-1.7796],\n",
      "        [-2.8587],\n",
      "        [-1.9271],\n",
      "        [ 0.8120]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.9461],\n",
      "        [ 1.4349],\n",
      "        [-3.1766],\n",
      "        [-1.6634],\n",
      "        [ 0.7205],\n",
      "        [ 0.0456],\n",
      "        [-1.4597],\n",
      "        [-1.2557],\n",
      "        [ 1.6406],\n",
      "        [ 1.0915],\n",
      "        [-2.7980],\n",
      "        [-0.8634],\n",
      "        [ 0.6364],\n",
      "        [-1.8981],\n",
      "        [-0.4764],\n",
      "        [ 0.7532],\n",
      "        [-1.6543],\n",
      "        [-2.6627],\n",
      "        [-2.2510],\n",
      "        [ 0.9095]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.0429],\n",
      "        [ 1.2655],\n",
      "        [-3.2426],\n",
      "        [-1.8399],\n",
      "        [ 0.5560],\n",
      "        [-0.0321],\n",
      "        [-1.6500],\n",
      "        [-1.3666],\n",
      "        [ 1.6392],\n",
      "        [ 1.1523],\n",
      "        [-2.8691],\n",
      "        [-0.9643],\n",
      "        [ 0.4270],\n",
      "        [-1.9561],\n",
      "        [-0.5547],\n",
      "        [ 0.7174],\n",
      "        [-1.5319],\n",
      "        [-2.4481],\n",
      "        [-2.4885],\n",
      "        [ 1.0652]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.0688],\n",
      "        [ 1.0083],\n",
      "        [-3.3088],\n",
      "        [-1.9931],\n",
      "        [ 0.3531],\n",
      "        [-0.0889],\n",
      "        [-1.8765],\n",
      "        [-1.5639],\n",
      "        [ 1.5723],\n",
      "        [ 1.1952],\n",
      "        [-2.9114],\n",
      "        [-1.1155],\n",
      "        [ 0.2268],\n",
      "        [-1.9368],\n",
      "        [-0.6840],\n",
      "        [ 0.6560],\n",
      "        [-1.4770],\n",
      "        [-2.2734],\n",
      "        [-2.5670],\n",
      "        [ 1.1485]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.1256],\n",
      "        [ 0.7932],\n",
      "        [-3.3379],\n",
      "        [-2.0810],\n",
      "        [ 0.1411],\n",
      "        [-0.0867],\n",
      "        [-2.0846],\n",
      "        [-1.7661],\n",
      "        [ 1.5275],\n",
      "        [ 1.2219],\n",
      "        [-2.9491],\n",
      "        [-1.3619],\n",
      "        [ 0.0366],\n",
      "        [-1.9387],\n",
      "        [-0.8123],\n",
      "        [ 0.5576],\n",
      "        [-1.4620],\n",
      "        [-2.0563],\n",
      "        [-2.4387],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 1.2077]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.1218],\n",
      "        [ 0.6774],\n",
      "        [-3.3464],\n",
      "        [-2.1758],\n",
      "        [-0.1012],\n",
      "        [-0.1709],\n",
      "        [-2.2771],\n",
      "        [-1.9659],\n",
      "        [ 1.4057],\n",
      "        [ 1.1751],\n",
      "        [-2.9689],\n",
      "        [-1.6923],\n",
      "        [-0.1332],\n",
      "        [-2.0661],\n",
      "        [-0.9262],\n",
      "        [ 0.4582],\n",
      "        [-1.4714],\n",
      "        [-1.8314],\n",
      "        [-2.2659],\n",
      "        [ 1.2513]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.1627],\n",
      "        [ 0.6287],\n",
      "        [-3.2730],\n",
      "        [-2.2420],\n",
      "        [-0.2880],\n",
      "        [-0.3829],\n",
      "        [-2.4652],\n",
      "        [-2.0849],\n",
      "        [ 1.2820],\n",
      "        [ 1.1581],\n",
      "        [-2.8655],\n",
      "        [-1.9200],\n",
      "        [-0.2257],\n",
      "        [-2.3058],\n",
      "        [-1.0309],\n",
      "        [ 0.3706],\n",
      "        [-1.4739],\n",
      "        [-1.6335],\n",
      "        [-2.1177],\n",
      "        [ 1.1767]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.0977],\n",
      "        [ 0.6336],\n",
      "        [-3.2499],\n",
      "        [-2.3617],\n",
      "        [-0.4797],\n",
      "        [-0.4925],\n",
      "        [-2.6556],\n",
      "        [-2.1651],\n",
      "        [ 1.1335],\n",
      "        [ 1.0653],\n",
      "        [-2.7508],\n",
      "        [-2.1686],\n",
      "        [-0.2116],\n",
      "        [-2.2973],\n",
      "        [-1.0952],\n",
      "        [ 0.3065],\n",
      "        [-1.5111],\n",
      "        [-1.4822],\n",
      "        [-2.0202],\n",
      "        [ 1.2321]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.1039],\n",
      "        [ 0.6806],\n",
      "        [-3.1716],\n",
      "        [-2.5588],\n",
      "        [-0.6532],\n",
      "        [-0.5856],\n",
      "        [-2.8886],\n",
      "        [-2.2286],\n",
      "        [ 1.0724],\n",
      "        [ 0.9756],\n",
      "        [-2.5735],\n",
      "        [-2.2859],\n",
      "        [-0.3095],\n",
      "        [-2.3027],\n",
      "        [-1.1100],\n",
      "        [ 0.3489],\n",
      "        [-1.5424],\n",
      "        [-1.4067],\n",
      "        [-1.8661],\n",
      "        [ 1.0913]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.0775],\n",
      "        [ 0.6840],\n",
      "        [-3.1320],\n",
      "        [-2.5618],\n",
      "        [-0.8733],\n",
      "        [-0.6833],\n",
      "        [-3.1153],\n",
      "        [-2.2731],\n",
      "        [ 1.0004],\n",
      "        [ 0.9088],\n",
      "        [-2.3973],\n",
      "        [-2.3119],\n",
      "        [-0.4633],\n",
      "        [-2.4452],\n",
      "        [-1.1135],\n",
      "        [ 0.3509],\n",
      "        [-1.5135],\n",
      "        [-1.3501],\n",
      "        [-1.7247],\n",
      "        [ 1.1501]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.0749],\n",
      "        [ 0.5942],\n",
      "        [-3.2098],\n",
      "        [-2.5041],\n",
      "        [-0.9616],\n",
      "        [-0.7378],\n",
      "        [-3.2545],\n",
      "        [-2.3837],\n",
      "        [ 0.8704],\n",
      "        [ 0.8760],\n",
      "        [-2.2879],\n",
      "        [-2.2187],\n",
      "        [-0.6228],\n",
      "        [-2.5951],\n",
      "        [-1.1098],\n",
      "        [ 0.3753],\n",
      "        [-1.5242],\n",
      "        [-1.3005],\n",
      "        [-1.6273],\n",
      "        [ 1.0104]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.0868],\n",
      "        [ 0.4671],\n",
      "        [-3.2145],\n",
      "        [-2.4889],\n",
      "        [-1.0884],\n",
      "        [-0.7562],\n",
      "        [-3.1753],\n",
      "        [-2.5451],\n",
      "        [ 0.7073],\n",
      "        [ 0.9603],\n",
      "        [-2.2325],\n",
      "        [-2.0033],\n",
      "        [-0.7170],\n",
      "        [-2.7373],\n",
      "        [-1.1248],\n",
      "        [ 0.4310],\n",
      "        [-1.5543],\n",
      "        [-1.2732],\n",
      "        [-1.4762],\n",
      "        [ 0.6349]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.1497],\n",
      "        [ 0.4037],\n",
      "        [-3.1449],\n",
      "        [-2.5940],\n",
      "        [-1.0850],\n",
      "        [-0.7759],\n",
      "        [-3.0634],\n",
      "        [-2.6094],\n",
      "        [ 0.5440],\n",
      "        [ 0.8666],\n",
      "        [-2.2085],\n",
      "        [-1.8328],\n",
      "        [-0.6986],\n",
      "        [-2.8071],\n",
      "        [-1.1125],\n",
      "        [ 0.4581],\n",
      "        [-1.5033],\n",
      "        [-1.2388],\n",
      "        [-1.4030],\n",
      "        [ 0.2740]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.2764],\n",
      "        [ 0.3802],\n",
      "        [-3.1450],\n",
      "        [-2.7972],\n",
      "        [-1.0071],\n",
      "        [-0.7773],\n",
      "        [-2.9498],\n",
      "        [-2.6562],\n",
      "        [ 0.3967],\n",
      "        [ 0.7330],\n",
      "        [-2.2072],\n",
      "        [-1.6801],\n",
      "        [-0.5733],\n",
      "        [-3.0205],\n",
      "        [-1.0777],\n",
      "        [ 0.4760],\n",
      "        [-1.5162],\n",
      "        [-1.1580],\n",
      "        [-1.3707],\n",
      "        [ 0.0173]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.4734],\n",
      "        [ 0.3235],\n",
      "        [-3.1681],\n",
      "        [-2.8038],\n",
      "        [-1.0309],\n",
      "        [-0.7386],\n",
      "        [-2.7261],\n",
      "        [-2.6570],\n",
      "        [ 0.2773],\n",
      "        [ 0.6130],\n",
      "        [-2.1955],\n",
      "        [-1.5114],\n",
      "        [-0.4956],\n",
      "        [-3.2382],\n",
      "        [-1.0413],\n",
      "        [ 0.4511],\n",
      "        [-1.5929],\n",
      "        [-1.1134],\n",
      "        [-1.3381],\n",
      "        [-0.1072]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.7030],\n",
      "        [ 0.1846],\n",
      "        [-3.2467],\n",
      "        [-2.8186],\n",
      "        [-1.1156],\n",
      "        [-0.6705],\n",
      "        [-2.5093],\n",
      "        [-2.6090],\n",
      "        [ 0.2030],\n",
      "        [ 0.4958],\n",
      "        [-2.1642],\n",
      "        [-1.3831],\n",
      "        [-0.4013],\n",
      "        [-3.5680],\n",
      "        [-0.9819],\n",
      "        [ 0.4029],\n",
      "        [-1.6527],\n",
      "        [-1.1365],\n",
      "        [-1.2880],\n",
      "        [-0.3035]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.9502],\n",
      "        [-0.0153],\n",
      "        [-3.3047],\n",
      "        [-2.8619],\n",
      "        [-1.3697],\n",
      "        [-0.5853],\n",
      "        [-2.2659],\n",
      "        [-2.5929],\n",
      "        [ 0.0761],\n",
      "        [ 0.4022],\n",
      "        [-2.1261],\n",
      "        [-1.4931],\n",
      "        [-0.3735],\n",
      "        [-3.8369],\n",
      "        [-0.8752],\n",
      "        [ 0.3124],\n",
      "        [-1.7069],\n",
      "        [-1.2018],\n",
      "        [-1.2403],\n",
      "        [-0.4779]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-2.1027],\n",
      "        [-0.2602],\n",
      "        [-3.3525],\n",
      "        [-2.9039],\n",
      "        [-1.4519],\n",
      "        [-0.5377],\n",
      "        [-2.0198],\n",
      "        [-2.6433],\n",
      "        [-0.0318],\n",
      "        [ 0.3083],\n",
      "        [-2.0745],\n",
      "        [-1.4859],\n",
      "        [-0.2939],\n",
      "        [-4.0868],\n",
      "        [-0.8526],\n",
      "        [ 0.2279],\n",
      "        [-1.7847],\n",
      "        [-1.3709],\n",
      "        [-1.2526],\n",
      "        [-0.6631]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-2.1504],\n",
      "        [-0.4132],\n",
      "        [-3.3838],\n",
      "        [-3.0636],\n",
      "        [-1.5559],\n",
      "        [-0.4813],\n",
      "        [-1.8562],\n",
      "        [-2.6337],\n",
      "        [-0.2014],\n",
      "        [ 0.2464],\n",
      "        [-2.0121],\n",
      "        [-1.2688],\n",
      "        [-0.2007],\n",
      "        [-4.4258],\n",
      "        [-0.7988],\n",
      "        [ 0.0948],\n",
      "        [-1.8536],\n",
      "        [-1.5108],\n",
      "        [-1.2359],\n",
      "        [-0.8657]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-2.1742],\n",
      "        [-0.5290],\n",
      "        [-3.3531],\n",
      "        [-3.1951],\n",
      "        [-1.6283],\n",
      "        [-0.4296],\n",
      "        [-1.7716],\n",
      "        [-2.5193],\n",
      "        [-0.3858],\n",
      "        [ 0.0878],\n",
      "        [-1.9559],\n",
      "        [-1.2873],\n",
      "        [-0.0173],\n",
      "        [-4.6014],\n",
      "        [-0.8276],\n",
      "        [-0.0265],\n",
      "        [-1.9100],\n",
      "        [-1.7108],\n",
      "        [-1.2083],\n",
      "        [-1.0019]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-2.1468],\n",
      "        [-0.7020],\n",
      "        [-3.3996],\n",
      "        [-3.4658],\n",
      "        [-1.6705],\n",
      "        [-0.4057],\n",
      "        [-1.6486],\n",
      "        [-2.3994],\n",
      "        [-0.5416],\n",
      "        [-0.1144],\n",
      "        [-1.8809],\n",
      "        [-1.2015],\n",
      "        [ 0.2044],\n",
      "        [-4.6084],\n",
      "        [-0.8711],\n",
      "        [-0.0821],\n",
      "        [-1.9535],\n",
      "        [-1.8375],\n",
      "        [-1.1869],\n",
      "        [-1.0952]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-2.0918],\n",
      "        [-0.8923],\n",
      "        [-3.4680],\n",
      "        [-3.7179],\n",
      "        [-1.6446],\n",
      "        [-0.4143],\n",
      "        [-1.5310],\n",
      "        [-2.2131],\n",
      "        [-0.7642],\n",
      "        [-0.3164],\n",
      "        [-1.7900],\n",
      "        [-1.0264],\n",
      "        [ 0.4501],\n",
      "        [-4.5236],\n",
      "        [-0.8989],\n",
      "        [-0.2477],\n",
      "        [-1.9670],\n",
      "        [-1.9163],\n",
      "        [-1.1908],\n",
      "        [-1.1643]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-2.0450],\n",
      "        [-1.1455],\n",
      "        [-3.5165],\n",
      "        [-3.8972],\n",
      "        [-1.7614],\n",
      "        [-0.4012],\n",
      "        [-1.4408],\n",
      "        [-2.1309],\n",
      "        [-0.7760],\n",
      "        [-0.5803],\n",
      "        [-1.6773],\n",
      "        [-1.0286],\n",
      "        [ 0.6961],\n",
      "        [-4.4060],\n",
      "        [-0.9234],\n",
      "        [-0.4474],\n",
      "        [-1.9960],\n",
      "        [-1.9739],\n",
      "        [-1.1742],\n",
      "        [-1.2582]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.8870],\n",
      "        [-1.3496],\n",
      "        [-3.6098],\n",
      "        [-4.1323],\n",
      "        [-1.8213],\n",
      "        [-0.4159],\n",
      "        [-1.3656],\n",
      "        [-2.0300],\n",
      "        [-0.8772],\n",
      "        [-0.8273],\n",
      "        [-1.6404],\n",
      "        [-0.8795],\n",
      "        [ 0.9532],\n",
      "        [-4.3003],\n",
      "        [-1.0102],\n",
      "        [-0.6083],\n",
      "        [-1.9989],\n",
      "        [-2.0445],\n",
      "        [-1.2195],\n",
      "        [-1.3901]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.7649],\n",
      "        [-1.5825],\n",
      "        [-3.7271],\n",
      "        [-4.3034],\n",
      "        [-1.8638],\n",
      "        [-0.3927],\n",
      "        [-1.3434],\n",
      "        [-1.9566],\n",
      "        [-0.9528],\n",
      "        [-1.0989],\n",
      "        [-1.6929],\n",
      "        [-0.9348],\n",
      "        [ 1.2215],\n",
      "        [-4.2653],\n",
      "        [-1.1103],\n",
      "        [-0.8517],\n",
      "        [-1.9935],\n",
      "        [-2.1385],\n",
      "        [-1.2740],\n",
      "        [-1.3354]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.6251],\n",
      "        [-1.6896],\n",
      "        [-3.8379],\n",
      "        [-4.4381],\n",
      "        [-1.9332],\n",
      "        [-0.4299],\n",
      "        [-1.3490],\n",
      "        [-1.8016],\n",
      "        [-0.9893],\n",
      "        [-1.2495],\n",
      "        [-1.7005],\n",
      "        [-0.9699],\n",
      "        [ 1.4876],\n",
      "        [-4.2634],\n",
      "        [-1.1714],\n",
      "        [-1.0905],\n",
      "        [-2.0186],\n",
      "        [-2.1227],\n",
      "        [-1.2829],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-1.0471]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.3893],\n",
      "        [-1.6890],\n",
      "        [-3.8658],\n",
      "        [-4.5504],\n",
      "        [-2.0047],\n",
      "        [-0.4500],\n",
      "        [-1.3840],\n",
      "        [-1.5827],\n",
      "        [-1.0094],\n",
      "        [-1.5387],\n",
      "        [-1.7341],\n",
      "        [-0.9032],\n",
      "        [ 1.7466],\n",
      "        [-4.3124],\n",
      "        [-1.2886],\n",
      "        [-1.2714],\n",
      "        [-2.0661],\n",
      "        [-2.1232],\n",
      "        [-1.3454],\n",
      "        [-0.7854]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-1.1568],\n",
      "        [-1.6215],\n",
      "        [-3.8693],\n",
      "        [-4.6532],\n",
      "        [-2.0392],\n",
      "        [-0.4453],\n",
      "        [-1.4474],\n",
      "        [-1.3251],\n",
      "        [-0.9521],\n",
      "        [-1.7806],\n",
      "        [-1.7319],\n",
      "        [-0.7069],\n",
      "        [ 1.9777],\n",
      "        [-4.2792],\n",
      "        [-1.3736],\n",
      "        [-1.4502],\n",
      "        [-2.1468],\n",
      "        [-2.1026],\n",
      "        [-1.3805],\n",
      "        [-0.5294]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.9019],\n",
      "        [-1.6024],\n",
      "        [-3.8710],\n",
      "        [-4.6352],\n",
      "        [-2.0889],\n",
      "        [-0.4720],\n",
      "        [-1.5263],\n",
      "        [-1.1035],\n",
      "        [-0.9136],\n",
      "        [-1.9619],\n",
      "        [-1.7091],\n",
      "        [-0.4620],\n",
      "        [ 2.1763],\n",
      "        [-4.2820],\n",
      "        [-1.5536],\n",
      "        [-1.6005],\n",
      "        [-2.2749],\n",
      "        [-2.0495],\n",
      "        [-1.4072],\n",
      "        [-0.2548]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-6.6163e-01],\n",
      "        [-1.4753e+00],\n",
      "        [-3.8674e+00],\n",
      "        [-4.5474e+00],\n",
      "        [-2.0830e+00],\n",
      "        [-3.6371e-01],\n",
      "        [-1.7009e+00],\n",
      "        [-8.8860e-01],\n",
      "        [-9.0530e-01],\n",
      "        [-2.1022e+00],\n",
      "        [-1.6768e+00],\n",
      "        [-2.9323e-01],\n",
      "        [ 2.3063e+00],\n",
      "        [-4.2323e+00],\n",
      "        [-1.7058e+00],\n",
      "        [-1.7015e+00],\n",
      "        [-2.4468e+00],\n",
      "        [-1.9484e+00],\n",
      "        [-1.4732e+00],\n",
      "        [-7.4595e-05]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.2817],\n",
      "        [-1.4379],\n",
      "        [-3.8276],\n",
      "        [-4.4099],\n",
      "        [-2.0763],\n",
      "        [-0.2831],\n",
      "        [-1.7050],\n",
      "        [-0.6783],\n",
      "        [-0.9080],\n",
      "        [-2.2332],\n",
      "        [-1.6394],\n",
      "        [-0.2431],\n",
      "        [ 2.3597],\n",
      "        [-4.2774],\n",
      "        [-1.8938],\n",
      "        [-1.7451],\n",
      "        [-2.5986],\n",
      "        [-1.9401],\n",
      "        [-1.5360],\n",
      "        [ 0.2524]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.1718],\n",
      "        [-1.3868],\n",
      "        [-3.7514],\n",
      "        [-4.3280],\n",
      "        [-2.0611],\n",
      "        [-0.2260],\n",
      "        [-1.9308],\n",
      "        [-0.5170],\n",
      "        [-1.0051],\n",
      "        [-2.3412],\n",
      "        [-1.5724],\n",
      "        [-0.2842],\n",
      "        [ 2.3836],\n",
      "        [-4.4120],\n",
      "        [-2.0232],\n",
      "        [-1.7713],\n",
      "        [-2.8041],\n",
      "        [-1.9592],\n",
      "        [-1.6224],\n",
      "        [ 0.4792]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.6168],\n",
      "        [-1.3480],\n",
      "        [-3.6034],\n",
      "        [-4.1632],\n",
      "        [-2.1247],\n",
      "        [-0.1989],\n",
      "        [-2.0274],\n",
      "        [-0.4290],\n",
      "        [-0.9927],\n",
      "        [-2.4587],\n",
      "        [-1.4807],\n",
      "        [-0.1082],\n",
      "        [ 2.3454],\n",
      "        [-4.4586],\n",
      "        [-2.1218],\n",
      "        [-1.7470],\n",
      "        [-2.9776],\n",
      "        [-1.7840],\n",
      "        [-1.7144],\n",
      "        [ 0.5687]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.0296],\n",
      "        [-1.2600],\n",
      "        [-3.3853],\n",
      "        [-4.2375],\n",
      "        [-2.0922],\n",
      "        [-0.2389],\n",
      "        [-2.1816],\n",
      "        [-0.3821],\n",
      "        [-0.9051],\n",
      "        [-2.5171],\n",
      "        [-1.4273],\n",
      "        [ 0.2447],\n",
      "        [ 2.3395],\n",
      "        [-4.4359],\n",
      "        [-2.2154],\n",
      "        [-1.7103],\n",
      "        [-3.0964],\n",
      "        [-1.4843],\n",
      "        [-1.8240],\n",
      "        [ 0.5788]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.3855],\n",
      "        [-1.1607],\n",
      "        [-3.2004],\n",
      "        [-4.2362],\n",
      "        [-1.9769],\n",
      "        [-0.2118],\n",
      "        [-2.2572],\n",
      "        [-0.2955],\n",
      "        [-0.8829],\n",
      "        [-2.5553],\n",
      "        [-1.3785],\n",
      "        [ 0.5533],\n",
      "        [ 2.3099],\n",
      "        [-4.3170],\n",
      "        [-2.3373],\n",
      "        [-1.7598],\n",
      "        [-3.2109],\n",
      "        [-1.2409],\n",
      "        [-1.9586],\n",
      "        [ 0.5839]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.6652],\n",
      "        [-1.1625],\n",
      "        [-3.0221],\n",
      "        [-3.9686],\n",
      "        [-1.8206],\n",
      "        [-0.1193],\n",
      "        [-2.1942],\n",
      "        [-0.2766],\n",
      "        [-0.8665],\n",
      "        [-2.5034],\n",
      "        [-1.3212],\n",
      "        [ 0.8754],\n",
      "        [ 2.3156],\n",
      "        [-4.2295],\n",
      "        [-2.3770],\n",
      "        [-1.8048],\n",
      "        [-3.3002],\n",
      "        [-0.9141],\n",
      "        [-2.1044],\n",
      "        [ 0.4684]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.9355],\n",
      "        [-1.2034],\n",
      "        [-2.8925],\n",
      "        [-3.8554],\n",
      "        [-1.6798],\n",
      "        [ 0.0360],\n",
      "        [-2.0853],\n",
      "        [-0.2445],\n",
      "        [-0.8554],\n",
      "        [-2.4163],\n",
      "        [-1.2617],\n",
      "        [ 1.1327],\n",
      "        [ 2.2970],\n",
      "        [-4.0648],\n",
      "        [-2.4818],\n",
      "        [-1.7119],\n",
      "        [-3.3729],\n",
      "        [-0.6048],\n",
      "        [-2.2163],\n",
      "        [ 0.3724]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.1524],\n",
      "        [-1.2920],\n",
      "        [-2.7544],\n",
      "        [-3.6189],\n",
      "        [-1.5877],\n",
      "        [ 0.1563],\n",
      "        [-1.9447],\n",
      "        [-0.2504],\n",
      "        [-0.8370],\n",
      "        [-2.3672],\n",
      "        [-1.1556],\n",
      "        [ 1.3748],\n",
      "        [ 2.3615],\n",
      "        [-3.8418],\n",
      "        [-2.6453],\n",
      "        [-1.6196],\n",
      "        [-3.4275],\n",
      "        [-0.4164],\n",
      "        [-2.3423],\n",
      "        [ 0.2458]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.3216],\n",
      "        [-1.3530],\n",
      "        [-2.6546],\n",
      "        [-3.3876],\n",
      "        [-1.5553],\n",
      "        [ 0.1396],\n",
      "        [-1.8837],\n",
      "        [-0.2712],\n",
      "        [-0.7879],\n",
      "        [-2.4040],\n",
      "        [-1.1166],\n",
      "        [ 1.5587],\n",
      "        [ 2.3540],\n",
      "        [-3.6509],\n",
      "        [-2.7965],\n",
      "        [-1.4235],\n",
      "        [-3.4261],\n",
      "        [-0.2768],\n",
      "        [-2.4718],\n",
      "        [ 0.0567]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.4077],\n",
      "        [-1.4337],\n",
      "        [-2.5635],\n",
      "        [-2.9827],\n",
      "        [-1.5902],\n",
      "        [ 0.1390],\n",
      "        [-1.8813],\n",
      "        [-0.2597],\n",
      "        [-0.7749],\n",
      "        [-2.5089],\n",
      "        [-1.0673],\n",
      "        [ 1.6981],\n",
      "        [ 2.3173],\n",
      "        [-3.5654],\n",
      "        [-2.9489],\n",
      "        [-1.2796],\n",
      "        [-3.3256],\n",
      "        [-0.1683],\n",
      "        [-2.5913],\n",
      "        [-0.1798]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.4117],\n",
      "        [-1.4413],\n",
      "        [-2.4885],\n",
      "        [-2.5780],\n",
      "        [-1.5825],\n",
      "        [ 0.1558],\n",
      "        [-1.8765],\n",
      "        [-0.2864],\n",
      "        [-0.7864],\n",
      "        [-2.6110],\n",
      "        [-1.0254],\n",
      "        [ 1.8033],\n",
      "        [ 2.2777],\n",
      "        [-3.6129],\n",
      "        [-3.0631],\n",
      "        [-1.2589],\n",
      "        [-3.1496],\n",
      "        [-0.0751],\n",
      "        [-2.6950],\n",
      "        [-0.5643]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.4639],\n",
      "        [-1.4442],\n",
      "        [-2.4637],\n",
      "        [-2.3162],\n",
      "        [-1.6474],\n",
      "        [ 0.1866],\n",
      "        [-1.9092],\n",
      "        [-0.2583],\n",
      "        [-0.7560],\n",
      "        [-2.6494],\n",
      "        [-0.9737],\n",
      "        [ 2.0475],\n",
      "        [ 2.2327],\n",
      "        [-3.6413],\n",
      "        [-3.1784],\n",
      "        [-1.1240],\n",
      "        [-3.0243],\n",
      "        [-0.0490],\n",
      "        [-2.7855],\n",
      "        [-0.8366]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.5095],\n",
      "        [-1.3928],\n",
      "        [-2.3892],\n",
      "        [-2.0396],\n",
      "        [-1.6422],\n",
      "        [ 0.1945],\n",
      "        [-1.9306],\n",
      "        [-0.2097],\n",
      "        [-0.7027],\n",
      "        [-2.6226],\n",
      "        [-0.9099],\n",
      "        [ 2.0801],\n",
      "        [ 2.2132],\n",
      "        [-3.6870],\n",
      "        [-3.2490],\n",
      "        [-0.8411],\n",
      "        [-2.8386],\n",
      "        [-0.0297],\n",
      "        [-2.8852],\n",
      "        [-1.2297]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.5159],\n",
      "        [-1.2875],\n",
      "        [-2.3015],\n",
      "        [-1.8682],\n",
      "        [-1.6822],\n",
      "        [ 0.1572],\n",
      "        [-1.9408],\n",
      "        [-0.1648],\n",
      "        [-0.6623],\n",
      "        [-2.5766],\n",
      "        [-0.8455],\n",
      "        [ 2.2843],\n",
      "        [ 2.2155],\n",
      "        [-3.6287],\n",
      "        [-3.2988],\n",
      "        [-0.6294],\n",
      "        [-2.7509],\n",
      "        [-0.0442],\n",
      "        [-2.9526],\n",
      "        [-1.5767]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.4991],\n",
      "        [-1.2681],\n",
      "        [-2.2394],\n",
      "        [-1.7615],\n",
      "        [-1.7287],\n",
      "        [ 0.0960],\n",
      "        [-1.9820],\n",
      "        [-0.2097],\n",
      "        [-0.6310],\n",
      "        [-2.4720],\n",
      "        [-0.7743],\n",
      "        [ 2.3574],\n",
      "        [ 2.1933],\n",
      "        [-3.4988],\n",
      "        [-3.3524],\n",
      "        [-0.4832],\n",
      "        [-2.7747],\n",
      "        [-0.1087],\n",
      "        [-3.0122],\n",
      "        [-2.0004]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.5494],\n",
      "        [-1.2792],\n",
      "        [-2.1529],\n",
      "        [-1.6326],\n",
      "        [-1.7880],\n",
      "        [ 0.0151],\n",
      "        [-2.1263],\n",
      "        [-0.1930],\n",
      "        [-0.6235],\n",
      "        [-2.3676],\n",
      "        [-0.6468],\n",
      "        [ 2.4250],\n",
      "        [ 2.1531],\n",
      "        [-3.2652],\n",
      "        [-3.3979],\n",
      "        [-0.4109],\n",
      "        [-2.7178],\n",
      "        [-0.1798],\n",
      "        [-2.9826],\n",
      "        [-2.3645]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.6022],\n",
      "        [-1.3662],\n",
      "        [-2.0680],\n",
      "        [-1.5577],\n",
      "        [-1.9413],\n",
      "        [-0.0716],\n",
      "        [-2.2683],\n",
      "        [-0.1355],\n",
      "        [-0.6259],\n",
      "        [-2.1895],\n",
      "        [-0.5757],\n",
      "        [ 2.4218],\n",
      "        [ 2.1309],\n",
      "        [-3.2055],\n",
      "        [-3.4710],\n",
      "        [-0.3176],\n",
      "        [-2.7107],\n",
      "        [-0.2684],\n",
      "        [-2.9256],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [-2.6312]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.6493],\n",
      "        [-1.4928],\n",
      "        [-2.0296],\n",
      "        [-1.5446],\n",
      "        [-2.1317],\n",
      "        [-0.1718],\n",
      "        [-2.4319],\n",
      "        [-0.1188],\n",
      "        [-0.5420],\n",
      "        [-1.9960],\n",
      "        [-0.5302],\n",
      "        [ 2.5230],\n",
      "        [ 2.0821],\n",
      "        [-3.0938],\n",
      "        [-3.5551],\n",
      "        [-0.2974],\n",
      "        [-2.6071],\n",
      "        [-0.3786],\n",
      "        [-2.8431],\n",
      "        [-2.8472]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.6966],\n",
      "        [-1.6412],\n",
      "        [-1.9974],\n",
      "        [-1.6235],\n",
      "        [-2.2535],\n",
      "        [-0.2614],\n",
      "        [-2.6025],\n",
      "        [-0.1776],\n",
      "        [-0.4637],\n",
      "        [-1.7850],\n",
      "        [-0.4703],\n",
      "        [ 2.6760],\n",
      "        [ 2.0591],\n",
      "        [-2.8949],\n",
      "        [-3.6310],\n",
      "        [-0.2539],\n",
      "        [-2.4506],\n",
      "        [-0.4745],\n",
      "        [-2.7013],\n",
      "        [-2.9755]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.7306],\n",
      "        [-1.7998],\n",
      "        [-1.9647],\n",
      "        [-1.7789],\n",
      "        [-2.3161],\n",
      "        [-0.2891],\n",
      "        [-2.7847],\n",
      "        [-0.2307],\n",
      "        [-0.3101],\n",
      "        [-1.7038],\n",
      "        [-0.4400],\n",
      "        [ 2.7321],\n",
      "        [ 1.9742],\n",
      "        [-2.6861],\n",
      "        [-3.6546],\n",
      "        [-0.1579],\n",
      "        [-2.3106],\n",
      "        [-0.5820],\n",
      "        [-2.6247],\n",
      "        [-3.0728]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.7598],\n",
      "        [-1.8791],\n",
      "        [-1.8920],\n",
      "        [-1.7863],\n",
      "        [-2.3989],\n",
      "        [-0.3124],\n",
      "        [-2.8323],\n",
      "        [-0.2783],\n",
      "        [-0.1955],\n",
      "        [-1.6722],\n",
      "        [-0.4107],\n",
      "        [ 2.7233],\n",
      "        [ 1.8578],\n",
      "        [-2.5798],\n",
      "        [-3.6895],\n",
      "        [ 0.0344],\n",
      "        [-2.2668],\n",
      "        [-0.6804],\n",
      "        [-2.5020],\n",
      "        [-3.1269]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.7542],\n",
      "        [-1.9991],\n",
      "        [-1.8045],\n",
      "        [-1.6896],\n",
      "        [-2.4948],\n",
      "        [-0.3469],\n",
      "        [-2.7965],\n",
      "        [-0.2955],\n",
      "        [-0.0730],\n",
      "        [-1.6587],\n",
      "        [-0.3731],\n",
      "        [ 2.5016],\n",
      "        [ 1.7077],\n",
      "        [-2.6294],\n",
      "        [-3.7215],\n",
      "        [ 0.2192],\n",
      "        [-2.1778],\n",
      "        [-0.7791],\n",
      "        [-2.3766],\n",
      "        [-3.2192]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.7832],\n",
      "        [-2.1185],\n",
      "        [-1.8359],\n",
      "        [-1.6099],\n",
      "        [-2.5924],\n",
      "        [-0.3939],\n",
      "        [-2.8268],\n",
      "        [-0.3603],\n",
      "        [ 0.0654],\n",
      "        [-1.6683],\n",
      "        [-0.3342],\n",
      "        [ 2.3262],\n",
      "        [ 1.5180],\n",
      "        [-2.5802],\n",
      "        [-3.8715],\n",
      "        [ 0.3417],\n",
      "        [-2.1423],\n",
      "        [-0.8631],\n",
      "        [-2.2371],\n",
      "        [-3.2671]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.7096],\n",
      "        [-2.2151],\n",
      "        [-1.9454],\n",
      "        [-1.6100],\n",
      "        [-2.6374],\n",
      "        [-0.4028],\n",
      "        [-2.8519],\n",
      "        [-0.4301],\n",
      "        [ 0.1759],\n",
      "        [-1.6857],\n",
      "        [-0.3143],\n",
      "        [ 2.2522],\n",
      "        [ 1.3357],\n",
      "        [-2.5361],\n",
      "        [-4.0434],\n",
      "        [ 0.4813],\n",
      "        [-2.0969],\n",
      "        [-0.9852],\n",
      "        [-2.1847],\n",
      "        [-3.2008]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.5407],\n",
      "        [-2.3501],\n",
      "        [-2.1298],\n",
      "        [-1.6159],\n",
      "        [-2.6313],\n",
      "        [-0.4050],\n",
      "        [-2.8474],\n",
      "        [-0.4612],\n",
      "        [ 0.2405],\n",
      "        [-1.6890],\n",
      "        [-0.2910],\n",
      "        [ 2.4238],\n",
      "        [ 1.1949],\n",
      "        [-2.5206],\n",
      "        [-4.1404],\n",
      "        [ 0.5629],\n",
      "        [-2.0343],\n",
      "        [-1.1271],\n",
      "        [-2.1386],\n",
      "        [-3.0959]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.3235],\n",
      "        [-2.4950],\n",
      "        [-2.2645],\n",
      "        [-1.6912],\n",
      "        [-2.6309],\n",
      "        [-0.3960],\n",
      "        [-2.8618],\n",
      "        [-0.5184],\n",
      "        [ 0.1983],\n",
      "        [-1.7242],\n",
      "        [-0.2600],\n",
      "        [ 2.4233],\n",
      "        [ 1.0852],\n",
      "        [-2.4886],\n",
      "        [-4.1980],\n",
      "        [ 0.5209],\n",
      "        [-1.9063],\n",
      "        [-1.2765],\n",
      "        [-2.1489],\n",
      "        [-3.0957]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 2.0585],\n",
      "        [-2.5946],\n",
      "        [-2.4272],\n",
      "        [-1.7263],\n",
      "        [-2.6127],\n",
      "        [-0.4046],\n",
      "        [-2.9086],\n",
      "        [-0.5761],\n",
      "        [ 0.1209],\n",
      "        [-1.7720],\n",
      "        [-0.1999],\n",
      "        [ 2.3331],\n",
      "        [ 1.0600],\n",
      "        [-2.4311],\n",
      "        [-4.1772],\n",
      "        [ 0.4896],\n",
      "        [-1.7558],\n",
      "        [-1.4412],\n",
      "        [-2.1735],\n",
      "        [-3.1489]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.7699e+00],\n",
      "        [-2.6135e+00],\n",
      "        [-2.4555e+00],\n",
      "        [-1.8467e+00],\n",
      "        [-2.5534e+00],\n",
      "        [-3.9298e-01],\n",
      "        [-2.9055e+00],\n",
      "        [-6.2454e-01],\n",
      "        [ 2.7876e-03],\n",
      "        [-1.7558e+00],\n",
      "        [-1.3292e-01],\n",
      "        [ 2.2576e+00],\n",
      "        [ 1.0578e+00],\n",
      "        [-2.3923e+00],\n",
      "        [-3.9577e+00],\n",
      "        [ 5.2157e-01],\n",
      "        [-1.6727e+00],\n",
      "        [-1.6428e+00],\n",
      "        [-2.0922e+00],\n",
      "        [-3.1994e+00]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.4981],\n",
      "        [-2.6203],\n",
      "        [-2.4740],\n",
      "        [-1.9607],\n",
      "        [-2.4964],\n",
      "        [-0.3682],\n",
      "        [-2.8908],\n",
      "        [-0.6895],\n",
      "        [-0.0930],\n",
      "        [-1.7160],\n",
      "        [-0.0601],\n",
      "        [ 2.2602],\n",
      "        [ 1.0697],\n",
      "        [-2.2734],\n",
      "        [-3.6959],\n",
      "        [ 0.6085],\n",
      "        [-1.7363],\n",
      "        [-1.8155],\n",
      "        [-2.0770],\n",
      "        [-3.2291]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.3625],\n",
      "        [-2.6258],\n",
      "        [-2.6379],\n",
      "        [-2.0520],\n",
      "        [-2.4512],\n",
      "        [-0.3362],\n",
      "        [-2.9011],\n",
      "        [-0.7269],\n",
      "        [-0.2507],\n",
      "        [-1.6587],\n",
      "        [ 0.0304],\n",
      "        [ 2.1193],\n",
      "        [ 1.1735],\n",
      "        [-2.0998],\n",
      "        [-3.3798],\n",
      "        [ 0.8088],\n",
      "        [-1.9504],\n",
      "        [-2.0004],\n",
      "        [-2.0103],\n",
      "        [-3.2706]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.2684],\n",
      "        [-2.5973],\n",
      "        [-2.6734],\n",
      "        [-2.1384],\n",
      "        [-2.3796],\n",
      "        [-0.2974],\n",
      "        [-2.9688],\n",
      "        [-0.8147],\n",
      "        [-0.3951],\n",
      "        [-1.7460],\n",
      "        [ 0.1284],\n",
      "        [ 2.0776],\n",
      "        [ 1.2234],\n",
      "        [-1.8656],\n",
      "        [-3.0981],\n",
      "        [ 1.0617],\n",
      "        [-2.0550],\n",
      "        [-2.1591],\n",
      "        [-2.0398],\n",
      "        [-3.3072]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.1564],\n",
      "        [-2.5676],\n",
      "        [-2.7542],\n",
      "        [-2.3576],\n",
      "        [-2.3464],\n",
      "        [-0.2675],\n",
      "        [-3.0579],\n",
      "        [-0.8672],\n",
      "        [-0.5660],\n",
      "        [-1.7395],\n",
      "        [ 0.2080],\n",
      "        [ 1.9640],\n",
      "        [ 1.2430],\n",
      "        [-1.6632],\n",
      "        [-2.8414],\n",
      "        [ 1.2034],\n",
      "        [-2.3327],\n",
      "        [-2.1952],\n",
      "        [-2.0682],\n",
      "        [-3.3720]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 1.0794],\n",
      "        [-2.3390],\n",
      "        [-2.6889],\n",
      "        [-2.5740],\n",
      "        [-2.3514],\n",
      "        [-0.2508],\n",
      "        [-3.1496],\n",
      "        [-0.8872],\n",
      "        [-0.6535],\n",
      "        [-1.7384],\n",
      "        [ 0.3082],\n",
      "        [ 1.9329],\n",
      "        [ 1.2428],\n",
      "        [-1.5038],\n",
      "        [-2.6232],\n",
      "        [ 1.3435],\n",
      "        [-2.4800],\n",
      "        [-2.3177],\n",
      "        [-2.0523],\n",
      "        [-3.4598]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.9898],\n",
      "        [-2.0084],\n",
      "        [-2.7255],\n",
      "        [-2.7537],\n",
      "        [-2.2535],\n",
      "        [-0.2339],\n",
      "        [-3.2752],\n",
      "        [-0.8489],\n",
      "        [-0.6782],\n",
      "        [-1.6671],\n",
      "        [ 0.4125],\n",
      "        [ 1.8598],\n",
      "        [ 1.1606],\n",
      "        [-1.3752],\n",
      "        [-2.4390],\n",
      "        [ 1.3642],\n",
      "        [-2.4191],\n",
      "        [-2.4620],\n",
      "        [-2.1230],\n",
      "        [-3.5474]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.8941],\n",
      "        [-1.6299],\n",
      "        [-2.6142],\n",
      "        [-2.7172],\n",
      "        [-2.1842],\n",
      "        [-0.2286],\n",
      "        [-3.4200],\n",
      "        [-0.8194],\n",
      "        [-0.6494],\n",
      "        [-1.5226],\n",
      "        [ 0.5197],\n",
      "        [ 1.8268],\n",
      "        [ 0.9872],\n",
      "        [-1.2558],\n",
      "        [-2.3369],\n",
      "        [ 1.4937],\n",
      "        [-2.3524],\n",
      "        [-2.5900],\n",
      "        [-2.2078],\n",
      "        [-3.6196]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.7898],\n",
      "        [-1.2893],\n",
      "        [-2.4301],\n",
      "        [-2.8820],\n",
      "        [-2.1439],\n",
      "        [-0.2256],\n",
      "        [-3.5690],\n",
      "        [-0.7622],\n",
      "        [-0.5622],\n",
      "        [-1.3746],\n",
      "        [ 0.5930],\n",
      "        [ 1.8376],\n",
      "        [ 0.8233],\n",
      "        [-1.1595],\n",
      "        [-2.2276],\n",
      "        [ 1.6445],\n",
      "        [-2.2195],\n",
      "        [-2.6415],\n",
      "        [-2.2710],\n",
      "        [-3.5664]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.6667],\n",
      "        [-0.9998],\n",
      "        [-2.3093],\n",
      "        [-3.0224],\n",
      "        [-2.1705],\n",
      "        [-0.2223],\n",
      "        [-3.7228],\n",
      "        [-0.7717],\n",
      "        [-0.4656],\n",
      "        [-1.2648],\n",
      "        [ 0.5707],\n",
      "        [ 1.8708],\n",
      "        [ 0.6996],\n",
      "        [-1.0980],\n",
      "        [-2.1591],\n",
      "        [ 1.8199],\n",
      "        [-2.0662],\n",
      "        [-2.6607],\n",
      "        [-2.1703],\n",
      "        [-3.4496]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.5346],\n",
      "        [-0.7350],\n",
      "        [-2.3099],\n",
      "        [-2.9755],\n",
      "        [-2.2066],\n",
      "        [-0.2104],\n",
      "        [-3.8064],\n",
      "        [-0.7313],\n",
      "        [-0.3866],\n",
      "        [-1.1873],\n",
      "        [ 0.4829],\n",
      "        [ 1.9220],\n",
      "        [ 0.4564],\n",
      "        [-1.0659],\n",
      "        [-2.0652],\n",
      "        [ 2.0205],\n",
      "        [-2.0276],\n",
      "        [-2.6218],\n",
      "        [-2.1346],\n",
      "        [-3.3384]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.3297],\n",
      "        [-0.5130],\n",
      "        [-2.4311],\n",
      "        [-2.9637],\n",
      "        [-2.2883],\n",
      "        [-0.2000],\n",
      "        [-3.8829],\n",
      "        [-0.6978],\n",
      "        [-0.3424],\n",
      "        [-1.2417],\n",
      "        [ 0.3208],\n",
      "        [ 2.0027],\n",
      "        [ 0.2803],\n",
      "        [-0.9973],\n",
      "        [-1.9094],\n",
      "        [ 2.1060],\n",
      "        [-2.0129],\n",
      "        [-2.5737],\n",
      "        [-1.9959],\n",
      "        [-3.2534]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[ 0.1263],\n",
      "        [-0.3231],\n",
      "        [-2.3674],\n",
      "        [-3.0149],\n",
      "        [-2.3528],\n",
      "        [-0.1995],\n",
      "        [-4.0010],\n",
      "        [-0.6817],\n",
      "        [-0.3267],\n",
      "        [-1.2594],\n",
      "        [ 0.2788],\n",
      "        [ 2.0513],\n",
      "        [ 0.2420],\n",
      "        [-0.9774],\n",
      "        [-1.8304],\n",
      "        [ 2.2027],\n",
      "        [-2.0513],\n",
      "        [-2.5706],\n",
      "        [-1.8643],\n",
      "        [-3.1479]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.0325],\n",
      "        [-0.1673],\n",
      "        [-2.2382],\n",
      "        [-3.1371],\n",
      "        [-2.4260],\n",
      "        [-0.2155],\n",
      "        [-4.1712],\n",
      "        [-0.6949],\n",
      "        [-0.3049],\n",
      "        [-1.1911],\n",
      "        [ 0.2062],\n",
      "        [ 2.1033],\n",
      "        [ 0.1718],\n",
      "        [-0.9572],\n",
      "        [-1.8327],\n",
      "        [ 2.1823],\n",
      "        [-2.0752],\n",
      "        [-2.5873],\n",
      "        [-1.7765],\n",
      "        [-3.0337]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.1856],\n",
      "        [ 0.0414],\n",
      "        [-2.1727],\n",
      "        [-3.3963],\n",
      "        [-2.4620],\n",
      "        [-0.2566],\n",
      "        [-4.2484],\n",
      "        [-0.7194],\n",
      "        [-0.3647],\n",
      "        [-1.1458],\n",
      "        [ 0.1655],\n",
      "        [ 2.1289],\n",
      "        [ 0.0743],\n",
      "        [-1.0327],\n",
      "        [-1.8037],\n",
      "        [ 2.1329],\n",
      "        [-2.1416],\n",
      "        [-2.6224],\n",
      "        [-1.7807],\n",
      "        [-2.9798]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.2900],\n",
      "        [ 0.1956],\n",
      "        [-2.2285],\n",
      "        [-3.5573],\n",
      "        [-2.4719],\n",
      "        [-0.2659],\n",
      "        [-4.2746],\n",
      "        [-0.7182],\n",
      "        [-0.3507],\n",
      "        [-1.1650],\n",
      "        [ 0.1361],\n",
      "        [ 2.0419],\n",
      "        [ 0.0306],\n",
      "        [-1.1187],\n",
      "        [-1.7264],\n",
      "        [ 2.1875],\n",
      "        [-2.2353],\n",
      "        [-2.6364],\n",
      "        [-1.6598],\n",
      "        [-2.9937]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.3613],\n",
      "        [ 0.2636],\n",
      "        [-2.3980],\n",
      "        [-3.5565],\n",
      "        [-2.4684],\n",
      "        [-0.3820],\n",
      "        [-4.3145],\n",
      "        [-0.6977],\n",
      "        [-0.4149],\n",
      "        [-1.1853],\n",
      "        [ 0.1319],\n",
      "        [ 1.9396],\n",
      "        [ 0.0153],\n",
      "        [-1.2286],\n",
      "        [-1.6698],\n",
      "        [ 2.0926],\n",
      "        [-2.3262],\n",
      "        [-2.6169],\n",
      "        [-1.6481],\n",
      "        [-2.8967]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t tensor([[-0.5155],\n",
      "        [ 0.3368],\n",
      "        [-2.3925],\n",
      "        [-3.5677],\n",
      "        [-2.4947],\n",
      "        [-0.4609],\n",
      "        [-4.4692],\n",
      "        [-0.6428],\n",
      "        [-0.4268],\n",
      "        [-1.1314],\n",
      "        [ 0.0728],\n",
      "        [ 1.7977],\n",
      "        [ 0.0150],\n",
      "        [-1.3404],\n",
      "        [-1.5547],\n",
      "        [ 2.0071],\n",
      "        [-2.3686],\n",
      "        [-2.5915],\n",
      "        [-1.7388],\n",
      "        [-2.8206]], device='cuda:0')\n",
      "Dones:\t\t 20  *  1\n",
      "TD Error:\t 20  *  1\n",
      "Next TD:\t 20  *  1\n",
      "rewards:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "rewards:\t 20  *  1\n",
      "NS Values:\t 20  *  1\n",
      "S Values:\t 20  *  1\n",
      "Dones:\t\t 20  *  1\n",
      "\n",
      "Last A:\t "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-be1bdbffddef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA2C_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;31m# plot the scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-129-be1bdbffddef>\u001b[0m in \u001b[0;36mA2C_run\u001b[1;34m(n_episodes, max_t)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m#print (values.requires_grad)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroll_log_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroll_entropies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroll_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroll_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroll_dones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroll_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroll_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mscores_window\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m# save most recent score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mall_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m# save most recent score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-128-9ae038437888>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, roll_log_probs, roll_entropies, roll_values, roll_rewards, roll_dones, roll_actions, roll_states, next_values)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mtd_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mGAMMA\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnext_state_values\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstate_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Last A:\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_advantages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Dones:\\t\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' * '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'TD Error:\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' * '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtd_error\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\austi\\miniconda3\\envs\\drlnd2\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# characters to replace unicode characters with.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\austi\\miniconda3\\envs\\drlnd2\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_default_dtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[0msuffixes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dtype='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m             \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\austi\\miniconda3\\envs\\drlnd2\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\austi\\miniconda3\\envs\\drlnd2\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mnonzero_finite_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mnonzero_finite_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;31m# no valid number, do nothing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size)\n",
    "ROLLOUT_LENGTH = 1024\n",
    "\n",
    "def A2C_run(n_episodes=250, max_t=int(1e5/ROLLOUT_LENGTH)):\n",
    "    \"\"\"Advantage Actor Critic Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "    \"\"\"\n",
    "    all_scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)      # last 100 scores\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        #for t in range(max_t):\n",
    "        #    roll_log_probs = []\n",
    "        #    roll_entropies = []\n",
    "        #    roll_values = []\n",
    "        #    roll_rewards = []\n",
    "        #    roll_dones = []\n",
    "        #    roll_actions = []\n",
    "        #    roll_states = []\n",
    "        roll_log_probs = []\n",
    "        roll_entropies = []\n",
    "        roll_values = []\n",
    "        roll_rewards = []\n",
    "        roll_dones = []\n",
    "        roll_actions = []\n",
    "        roll_states = []\n",
    "        for _ in range(ROLLOUT_LENGTH):\n",
    "            actions, log_probs, entropies, values = agent.act(states, train=False)\n",
    "            roll_log_probs.append(log_probs)\n",
    "            roll_entropies.append(entropies)\n",
    "            roll_values.append(values)\n",
    "            roll_actions.append(actions)\n",
    "            roll_states.append(states)\n",
    "            #print (log_probs)\n",
    "            #actions = np.clip(actions, -1, 1)              # all actions between -1 and 1            \n",
    "            #actions = actions.astype(int)\n",
    "            env_info = env.step(actions.cpu().data.numpy())[brain_name]       # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            roll_rewards.append(rewards)\n",
    "            roll_dones.append(dones)\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        actions, log_probs, entropies, values = agent.act(states, train=False)\n",
    "        #print (values.requires_grad)\n",
    "        agent.step(roll_log_probs, roll_entropies, roll_values, roll_rewards, roll_dones, roll_actions, roll_states, values)\n",
    "        scores_window.append(np.mean(scores))        # save most recent score\n",
    "        all_scores.append(np.mean(scores))           # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tLast Score: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(scores)), end='')\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tLast Score: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(scores)))\n",
    "        if np.mean(scores_window)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.network.state_dict(), 'actor_checkpoint.pth')\n",
    "            break\n",
    "    return all_scores\n",
    "\n",
    "scores = A2C_run()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "for _ in range(5):\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    print (actions)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
