{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models for the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action, action distribution (for continuous spaces).\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "        \n",
    "        \n",
    "class Value(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Value, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> value.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class A2C(nn.Module):\n",
    "    \"\"\"A2C Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, std=0.0):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(A2C, self).__init__()\n",
    "        \n",
    "        self.actor = Policy(state_size, action_size)\n",
    "        self.critic = Value(state_size)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.zeros(1, action_size))\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action, action distribution (for continuous spaces).\"\"\"\n",
    "        mean = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        \n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99              # discount factor\n",
    "TAU = 0.95                # soft update control\n",
    "ACTOR_LR = 3e-4           # actor learning rate\n",
    "OPTIMIZATION_EPOCHS = 10  # PPO optimization iterations\n",
    "BATCH_SIZE = 32           # PPO sample size\n",
    "PPO_CLIP = .2             # PPO update clipping\n",
    "GRADIENT_CLIP = 5         # controlling size of gradients\n",
    "EPS = 1e-5                # optimizer EPS\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print (device)\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Networks\n",
    "        self.network = A2C(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=ACTOR_LR)\n",
    "        \n",
    "    def step(self, roll_log_probs, roll_values, roll_rewards, roll_dones, roll_actions, roll_states):\n",
    "        \"\"\"Updates actor network with loss of: advantage * log_probability + entropy\n",
    "           Updates critic network with loss of: (TD Error)^2\n",
    "           \n",
    "        Params\n",
    "        ======\n",
    "            states (array_like): current states of all parallel agents\n",
    "            next_states (array_like): next states of all parallel agents\n",
    "            actions (array_like): chosen actions of all parallel agents\n",
    "            dists (Torch object): action distributions of all chosen actions\n",
    "            rewards (array_like): rewards observed for all parallel agents\n",
    "            dones (array_like): booleans (True if terminal state, False otherwise)\n",
    "        \"\"\"        \n",
    "        # Initialize tensors\n",
    "        next_td = roll_values[-1]\n",
    "        advantages = torch.tensor([]).float().to(device)\n",
    "        log_probs_old = torch.tensor([]).float().to(device)\n",
    "        values = torch.tensor([]).float().to(device)\n",
    "        td_estimates = torch.tensor([]).float().to(device)\n",
    "        states = torch.tensor([]).float().to(device)\n",
    "        actions = torch.tensor([]).float().to(device)\n",
    "        last_advantages = torch.Tensor(np.zeros((20, 1))).float().to(device)\n",
    "        for i in reversed(range(len(roll_log_probs))):\n",
    "            # Initialize tensors\n",
    "            rewards = torch.Tensor(roll_rewards[i]).unsqueeze(1).float().to(device)\n",
    "            state_values = roll_values[i]\n",
    "            next_state_values = roll_values[i+1]\n",
    "            dones = torch.Tensor(roll_dones[i]).unsqueeze(1).float().to(device)\n",
    "            i_log_probs = roll_log_probs[i]\n",
    "            i_actions = roll_actions[i]\n",
    "            i_states = torch.from_numpy(roll_states[i]).float().to(device)\n",
    "            \n",
    "            # Compute TD Estimates and action advantages\n",
    "            next_td = rewards + (GAMMA * next_td * (1 - dones))\n",
    "            td_error = rewards + (GAMMA * next_state_values) - state_values\n",
    "            i_advantages = last_advantages * TAU * GAMMA * (1 - dones) + td_error\n",
    "            last_advantages = i_advantages\n",
    "            \n",
    "            # Concatenate tensors\n",
    "            advantages = torch.cat((advantages, i_advantages.detach()), 0)\n",
    "            log_probs_old = torch.cat((log_probs_old, i_log_probs.detach()), 0)\n",
    "            values = torch.cat((values, roll_values[i]), 0)\n",
    "            td_estimates = torch.cat((td_estimates, next_td), 0)\n",
    "            actions = torch.cat((actions, i_actions), 0)\n",
    "            states = torch.cat((states, i_states))\n",
    "            \n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "        self.learn(states, actions, log_probs_old, td_estimates, advantages)\n",
    "        \n",
    "    def learn (self, states, actions, log_probs_old, td_estimates, advantages):\n",
    "        for _ in range(OPTIMIZATION_EPOCHS):\n",
    "            for _ in range(states.size(0) // BATCH_SIZE):\n",
    "                sample_indices = np.random.randint(0, states.size(0), BATCH_SIZE)\n",
    "                sampled_states = states[sample_indices, :]\n",
    "                sampled_actions = actions[sample_indices, :]\n",
    "                sampled_log_probs_old = log_probs_old[sample_indices, :]\n",
    "                sampled_td_estimates = td_estimates[sample_indices, :]\n",
    "                sampled_advantages = advantages[sample_indices, :]\n",
    "            \n",
    "                dist, p_values = self.network(sampled_states)\n",
    "                p_log_probs = dist.log_prob(sampled_actions)\n",
    "                p_log_probs = torch.sum(p_log_probs, dim=1, keepdim=True)\n",
    "                p_entropies = dist.entropy().mean()\n",
    "                ratio = (p_log_probs - sampled_log_probs_old).exp()\n",
    "                sur = ratio * sampled_advantages\n",
    "                clipped_sur = ratio.clamp(1.0 - PPO_CLIP, 1.0 + PPO_CLIP) * sampled_advantages\n",
    "                policy_loss = -torch.min(sur, clipped_sur).mean(0) - .01 * p_entropies\n",
    "                value_loss = (sampled_td_estimates - p_values).pow(2).mean()\n",
    "            \n",
    "                self.optimizer.zero_grad()\n",
    "                (policy_loss + .5 * value_loss).backward()\n",
    "                nn.utils.clip_grad_norm_(self.network.parameters(), GRADIENT_CLIP)\n",
    "                self.optimizer.step()\n",
    "        \n",
    "    def act(self, states, train=False):\n",
    "        \"\"\"Returns action and action distribution for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "        \"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        if train:\n",
    "            dist, values = self.network(states)\n",
    "        else:\n",
    "            self.network.eval()\n",
    "            with torch.no_grad():\n",
    "                dist, values = self.network(states)\n",
    "            self.network.train()\n",
    "        actions = dist.sample()\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        log_probs = torch.sum(log_probs, dim=1, keepdim=True)\n",
    "        return actions, log_probs, values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher2020_Windows_x86_64\\\\Reacher.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\austi\\miniconda3\\envs\\drlnd2\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.29\tLast Score: 0.29\n",
      "Episode 2\tAverage Score: 0.28\tLast Score: 0.26\n",
      "Episode 3\tAverage Score: 0.25\tLast Score: 0.20\n",
      "Episode 4\tAverage Score: 0.28\tLast Score: 0.37\n",
      "Episode 5\tAverage Score: 0.30\tLast Score: 0.39\n",
      "Episode 6\tAverage Score: 0.35\tLast Score: 0.60\n",
      "Episode 7\tAverage Score: 0.39\tLast Score: 0.64\n",
      "Episode 8\tAverage Score: 0.48\tLast Score: 1.13\n",
      "Episode 9\tAverage Score: 0.55\tLast Score: 1.04\n",
      "Episode 10\tAverage Score: 0.63\tLast Score: 1.40\n",
      "Episode 10\tAverage Score: 0.63\tLast Score: 1.40\n",
      "Episode 11\tAverage Score: 0.69\tLast Score: 1.27\n",
      "Episode 12\tAverage Score: 0.78\tLast Score: 1.79\n",
      "Episode 13\tAverage Score: 0.85\tLast Score: 1.66\n",
      "Episode 14\tAverage Score: 0.91\tLast Score: 1.63\n",
      "Episode 15\tAverage Score: 0.99\tLast Score: 2.18\n",
      "Episode 16\tAverage Score: 1.06\tLast Score: 2.11\n",
      "Episode 17\tAverage Score: 1.14\tLast Score: 2.39\n",
      "Episode 18\tAverage Score: 1.20\tLast Score: 2.30\n",
      "Episode 19\tAverage Score: 1.29\tLast Score: 2.85\n",
      "Episode 20\tAverage Score: 1.34\tLast Score: 2.26\n",
      "Episode 20\tAverage Score: 1.34\tLast Score: 2.26\n",
      "Episode 21\tAverage Score: 1.42\tLast Score: 2.97\n",
      "Episode 22\tAverage Score: 1.48\tLast Score: 2.85\n",
      "Episode 23\tAverage Score: 1.57\tLast Score: 3.43\n",
      "Episode 24\tAverage Score: 1.64\tLast Score: 3.24\n",
      "Episode 25\tAverage Score: 1.70\tLast Score: 3.17\n",
      "Episode 26\tAverage Score: 1.76\tLast Score: 3.41\n",
      "Episode 27\tAverage Score: 1.81\tLast Score: 3.09\n",
      "Episode 28\tAverage Score: 1.90\tLast Score: 4.20\n",
      "Episode 29\tAverage Score: 1.96\tLast Score: 3.78\n",
      "Episode 30\tAverage Score: 2.03\tLast Score: 3.84\n",
      "Episode 30\tAverage Score: 2.03\tLast Score: 3.84\n",
      "Episode 31\tAverage Score: 2.10\tLast Score: 4.32\n",
      "Episode 32\tAverage Score: 2.18\tLast Score: 4.67\n",
      "Episode 33\tAverage Score: 2.26\tLast Score: 4.76\n",
      "Episode 34\tAverage Score: 2.33\tLast Score: 4.72\n",
      "Episode 35\tAverage Score: 2.41\tLast Score: 4.93\n",
      "Episode 36\tAverage Score: 2.47\tLast Score: 4.75\n",
      "Episode 37\tAverage Score: 2.55\tLast Score: 5.56\n",
      "Episode 38\tAverage Score: 2.61\tLast Score: 4.51\n",
      "Episode 39\tAverage Score: 2.68\tLast Score: 5.49\n",
      "Episode 40\tAverage Score: 2.73\tLast Score: 4.82\n",
      "Episode 40\tAverage Score: 2.73\tLast Score: 4.82\n",
      "Episode 41\tAverage Score: 2.79\tLast Score: 5.14\n",
      "Episode 42\tAverage Score: 2.87\tLast Score: 5.98\n",
      "Episode 43\tAverage Score: 2.94\tLast Score: 5.94\n",
      "Episode 44\tAverage Score: 3.00\tLast Score: 5.48\n",
      "Episode 45\tAverage Score: 3.07\tLast Score: 6.11\n",
      "Episode 46\tAverage Score: 3.14\tLast Score: 6.32\n",
      "Episode 47\tAverage Score: 3.20\tLast Score: 5.98\n",
      "Episode 48\tAverage Score: 3.26\tLast Score: 6.25\n",
      "Episode 49\tAverage Score: 3.34\tLast Score: 7.18\n",
      "Episode 50\tAverage Score: 3.41\tLast Score: 6.76\n",
      "Episode 50\tAverage Score: 3.41\tLast Score: 6.76\n",
      "Episode 51\tAverage Score: 3.49\tLast Score: 7.54\n",
      "Episode 52\tAverage Score: 3.56\tLast Score: 7.15\n",
      "Episode 53\tAverage Score: 3.62\tLast Score: 6.75\n",
      "Episode 54\tAverage Score: 3.69\tLast Score: 7.51\n",
      "Episode 55\tAverage Score: 3.77\tLast Score: 8.09\n",
      "Episode 56\tAverage Score: 3.84\tLast Score: 7.52\n",
      "Episode 57\tAverage Score: 3.90\tLast Score: 7.13\n",
      "Episode 58\tAverage Score: 3.97\tLast Score: 8.35\n",
      "Episode 59\tAverage Score: 4.04\tLast Score: 7.72\n",
      "Episode 60\tAverage Score: 4.12\tLast Score: 8.99\n",
      "Episode 60\tAverage Score: 4.12\tLast Score: 8.99\n",
      "Episode 61\tAverage Score: 4.19\tLast Score: 8.45\n",
      "Episode 62\tAverage Score: 4.25\tLast Score: 8.05\n",
      "Episode 63\tAverage Score: 4.31\tLast Score: 7.78\n",
      "Episode 64\tAverage Score: 4.37\tLast Score: 8.34\n",
      "Episode 65\tAverage Score: 4.44\tLast Score: 8.98\n",
      "Episode 66\tAverage Score: 4.50\tLast Score: 7.88\n",
      "Episode 67\tAverage Score: 4.56\tLast Score: 8.93\n",
      "Episode 68\tAverage Score: 4.62\tLast Score: 8.72\n",
      "Episode 69\tAverage Score: 4.68\tLast Score: 8.65\n",
      "Episode 70\tAverage Score: 4.74\tLast Score: 9.12\n",
      "Episode 70\tAverage Score: 4.74\tLast Score: 9.12\n",
      "Episode 71\tAverage Score: 4.81\tLast Score: 9.46\n",
      "Episode 72\tAverage Score: 4.88\tLast Score: 9.92\n",
      "Episode 73\tAverage Score: 4.95\tLast Score: 10.02\n",
      "Episode 74\tAverage Score: 5.01\tLast Score: 9.37\n",
      "Episode 75\tAverage Score: 5.07\tLast Score: 9.46\n",
      "Episode 76\tAverage Score: 5.13\tLast Score: 9.86\n",
      "Episode 77\tAverage Score: 5.18\tLast Score: 8.92\n",
      "Episode 78\tAverage Score: 5.24\tLast Score: 9.34\n",
      "Episode 79\tAverage Score: 5.29\tLast Score: 9.12\n",
      "Episode 80\tAverage Score: 5.35\tLast Score: 10.64\n",
      "Episode 80\tAverage Score: 5.35\tLast Score: 10.64\n",
      "Episode 81\tAverage Score: 5.41\tLast Score: 10.17\n",
      "Episode 82\tAverage Score: 5.48\tLast Score: 11.29\n",
      "Episode 83\tAverage Score: 5.54\tLast Score: 9.98\n",
      "Episode 84\tAverage Score: 5.60\tLast Score: 10.99\n",
      "Episode 85\tAverage Score: 5.68\tLast Score: 11.82\n",
      "Episode 86\tAverage Score: 5.75\tLast Score: 12.19\n",
      "Episode 87\tAverage Score: 5.82\tLast Score: 11.66\n",
      "Episode 88\tAverage Score: 5.90\tLast Score: 12.58\n",
      "Episode 89\tAverage Score: 5.97\tLast Score: 12.81\n",
      "Episode 90\tAverage Score: 6.06\tLast Score: 13.67\n",
      "Episode 90\tAverage Score: 6.06\tLast Score: 13.67\n",
      "Episode 91\tAverage Score: 6.15\tLast Score: 14.14\n",
      "Episode 92\tAverage Score: 6.23\tLast Score: 14.01\n",
      "Episode 93\tAverage Score: 6.32\tLast Score: 14.18\n",
      "Episode 94\tAverage Score: 6.41\tLast Score: 15.04\n",
      "Episode 95\tAverage Score: 6.50\tLast Score: 15.01\n",
      "Episode 96\tAverage Score: 6.59\tLast Score: 15.21\n",
      "Episode 97\tAverage Score: 6.68\tLast Score: 14.53\n",
      "Episode 98\tAverage Score: 6.76\tLast Score: 15.30\n",
      "Episode 99\tAverage Score: 6.85\tLast Score: 15.28\n",
      "Episode 100\tAverage Score: 6.93\tLast Score: 15.38\n",
      "Episode 100\tAverage Score: 6.93\tLast Score: 15.38\n",
      "Episode 101\tAverage Score: 7.10\tLast Score: 16.58\n",
      "Episode 102\tAverage Score: 7.26\tLast Score: 16.03\n",
      "Episode 103\tAverage Score: 7.43\tLast Score: 17.53\n",
      "Episode 104\tAverage Score: 7.58\tLast Score: 15.78\n",
      "Episode 105\tAverage Score: 7.76\tLast Score: 18.21\n",
      "Episode 106\tAverage Score: 7.93\tLast Score: 17.43\n",
      "Episode 107\tAverage Score: 8.10\tLast Score: 17.77\n",
      "Episode 108\tAverage Score: 8.28\tLast Score: 19.60\n",
      "Episode 109\tAverage Score: 8.46\tLast Score: 18.74\n",
      "Episode 110\tAverage Score: 8.64\tLast Score: 19.02\n",
      "Episode 110\tAverage Score: 8.64\tLast Score: 19.02\n",
      "Episode 111\tAverage Score: 8.79\tLast Score: 16.71\n",
      "Episode 112\tAverage Score: 8.95\tLast Score: 17.73\n",
      "Episode 113\tAverage Score: 9.13\tLast Score: 19.96\n",
      "Episode 114\tAverage Score: 9.31\tLast Score: 18.95\n",
      "Episode 115\tAverage Score: 9.48\tLast Score: 19.06\n",
      "Episode 116\tAverage Score: 9.67\tLast Score: 21.75\n",
      "Episode 117\tAverage Score: 9.84\tLast Score: 19.28\n",
      "Episode 118\tAverage Score: 10.03\tLast Score: 20.89\n",
      "Episode 119\tAverage Score: 10.19\tLast Score: 19.42\n",
      "Episode 120\tAverage Score: 10.38\tLast Score: 21.21\n",
      "Episode 120\tAverage Score: 10.38\tLast Score: 21.21\n",
      "Episode 121\tAverage Score: 10.55\tLast Score: 19.22\n",
      "Episode 122\tAverage Score: 10.72\tLast Score: 19.96\n",
      "Episode 123\tAverage Score: 10.90\tLast Score: 21.54\n",
      "Episode 124\tAverage Score: 11.08\tLast Score: 21.87\n",
      "Episode 125\tAverage Score: 11.28\tLast Score: 22.55\n",
      "Episode 126\tAverage Score: 11.47\tLast Score: 22.89\n",
      "Episode 127\tAverage Score: 11.66\tLast Score: 22.17\n",
      "Episode 128\tAverage Score: 11.84\tLast Score: 21.92\n",
      "Episode 129\tAverage Score: 12.03\tLast Score: 23.11\n",
      "Episode 130\tAverage Score: 12.23\tLast Score: 23.52\n",
      "Episode 130\tAverage Score: 12.23\tLast Score: 23.52\n",
      "Episode 131\tAverage Score: 12.42\tLast Score: 23.60\n",
      "Episode 132\tAverage Score: 12.61\tLast Score: 23.51\n",
      "Episode 133\tAverage Score: 12.79\tLast Score: 22.27\n",
      "Episode 134\tAverage Score: 12.97\tLast Score: 23.47\n",
      "Episode 135\tAverage Score: 13.17\tLast Score: 24.52\n",
      "Episode 136\tAverage Score: 13.36\tLast Score: 24.04\n",
      "Episode 137\tAverage Score: 13.55\tLast Score: 23.83\n",
      "Episode 138\tAverage Score: 13.73\tLast Score: 23.36\n",
      "Episode 139\tAverage Score: 13.92\tLast Score: 24.48\n",
      "Episode 140\tAverage Score: 14.13\tLast Score: 25.08\n",
      "Episode 140\tAverage Score: 14.13\tLast Score: 25.08\n",
      "Episode 141\tAverage Score: 14.33\tLast Score: 25.30\n",
      "Episode 142\tAverage Score: 14.52\tLast Score: 24.95\n",
      "Episode 143\tAverage Score: 14.72\tLast Score: 26.19\n",
      "Episode 144\tAverage Score: 14.92\tLast Score: 25.01\n",
      "Episode 145\tAverage Score: 15.10\tLast Score: 24.84\n",
      "Episode 146\tAverage Score: 15.31\tLast Score: 26.64\n",
      "Episode 147\tAverage Score: 15.51\tLast Score: 26.03\n",
      "Episode 148\tAverage Score: 15.70\tLast Score: 25.83\n",
      "Episode 149\tAverage Score: 15.89\tLast Score: 25.67\n",
      "Episode 150\tAverage Score: 16.09\tLast Score: 26.76\n",
      "Episode 150\tAverage Score: 16.09\tLast Score: 26.76\n",
      "Episode 151\tAverage Score: 16.26\tLast Score: 25.03\n",
      "Episode 152\tAverage Score: 16.45\tLast Score: 26.12\n",
      "Episode 153\tAverage Score: 16.65\tLast Score: 26.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 154\tAverage Score: 16.84\tLast Score: 27.22\n",
      "Episode 155\tAverage Score: 17.03\tLast Score: 27.03\n",
      "Episode 156\tAverage Score: 17.20\tLast Score: 24.73\n",
      "Episode 157\tAverage Score: 17.40\tLast Score: 26.42\n",
      "Episode 158\tAverage Score: 17.57\tLast Score: 25.17\n",
      "Episode 159\tAverage Score: 17.75\tLast Score: 26.18\n",
      "Episode 160\tAverage Score: 17.93\tLast Score: 26.96\n",
      "Episode 160\tAverage Score: 17.93\tLast Score: 26.96\n",
      "Episode 161\tAverage Score: 18.12\tLast Score: 27.02\n",
      "Episode 162\tAverage Score: 18.30\tLast Score: 26.58\n",
      "Episode 163\tAverage Score: 18.50\tLast Score: 27.83\n",
      "Episode 164\tAverage Score: 18.66\tLast Score: 24.52\n",
      "Episode 165\tAverage Score: 18.86\tLast Score: 28.52\n",
      "Episode 166\tAverage Score: 19.05\tLast Score: 27.20\n",
      "Episode 167\tAverage Score: 19.23\tLast Score: 26.71\n",
      "Episode 168\tAverage Score: 19.42\tLast Score: 27.59\n",
      "Episode 169\tAverage Score: 19.62\tLast Score: 28.72\n",
      "Episode 170\tAverage Score: 19.81\tLast Score: 27.98\n",
      "Episode 170\tAverage Score: 19.81\tLast Score: 27.98\n",
      "Episode 171\tAverage Score: 19.99\tLast Score: 27.96\n",
      "Episode 172\tAverage Score: 20.16\tLast Score: 26.89\n",
      "Episode 173\tAverage Score: 20.34\tLast Score: 27.93\n",
      "Episode 174\tAverage Score: 20.52\tLast Score: 27.03\n",
      "Episode 175\tAverage Score: 20.72\tLast Score: 29.48\n",
      "Episode 176\tAverage Score: 20.91\tLast Score: 29.14\n",
      "Episode 177\tAverage Score: 21.10\tLast Score: 27.52\n",
      "Episode 178\tAverage Score: 21.28\tLast Score: 27.72\n",
      "Episode 179\tAverage Score: 21.47\tLast Score: 27.77\n",
      "Episode 180\tAverage Score: 21.63\tLast Score: 27.16\n",
      "Episode 180\tAverage Score: 21.63\tLast Score: 27.16\n",
      "Episode 181\tAverage Score: 21.80\tLast Score: 27.18\n",
      "Episode 182\tAverage Score: 21.97\tLast Score: 28.37\n",
      "Episode 183\tAverage Score: 22.15\tLast Score: 27.56\n",
      "Episode 184\tAverage Score: 22.31\tLast Score: 26.95\n",
      "Episode 185\tAverage Score: 22.43\tLast Score: 23.80\n",
      "Episode 186\tAverage Score: 22.58\tLast Score: 27.56\n",
      "Episode 187\tAverage Score: 22.73\tLast Score: 26.24\n",
      "Episode 188\tAverage Score: 22.86\tLast Score: 26.25\n",
      "Episode 189\tAverage Score: 23.01\tLast Score: 27.07\n",
      "Episode 190\tAverage Score: 23.15\tLast Score: 27.60\n",
      "Episode 190\tAverage Score: 23.15\tLast Score: 27.60\n",
      "Episode 191\tAverage Score: 23.27\tLast Score: 26.18\n",
      "Episode 192\tAverage Score: 23.40\tLast Score: 27.39\n",
      "Episode 193\tAverage Score: 23.55\tLast Score: 29.37\n",
      "Episode 194\tAverage Score: 23.67\tLast Score: 26.75\n",
      "Episode 195\tAverage Score: 23.81\tLast Score: 29.35\n",
      "Episode 196\tAverage Score: 23.95\tLast Score: 28.96\n",
      "Episode 197\tAverage Score: 24.11\tLast Score: 30.17\n",
      "Episode 198\tAverage Score: 24.24\tLast Score: 28.94\n",
      "Episode 199\tAverage Score: 24.37\tLast Score: 27.66\n",
      "Episode 200\tAverage Score: 24.50\tLast Score: 28.33\n",
      "Episode 200\tAverage Score: 24.50\tLast Score: 28.33\n",
      "Episode 201\tAverage Score: 24.62\tLast Score: 29.08\n",
      "Episode 202\tAverage Score: 24.75\tLast Score: 29.13\n",
      "Episode 203\tAverage Score: 24.87\tLast Score: 29.47\n",
      "Episode 204\tAverage Score: 24.99\tLast Score: 27.59\n",
      "Episode 205\tAverage Score: 25.09\tLast Score: 28.22\n",
      "Episode 206\tAverage Score: 25.19\tLast Score: 27.75\n",
      "Episode 207\tAverage Score: 25.30\tLast Score: 28.91\n",
      "Episode 208\tAverage Score: 25.39\tLast Score: 28.26\n",
      "Episode 209\tAverage Score: 25.49\tLast Score: 28.26\n",
      "Episode 210\tAverage Score: 25.59\tLast Score: 29.06\n",
      "Episode 210\tAverage Score: 25.59\tLast Score: 29.06\n",
      "Episode 211\tAverage Score: 25.70\tLast Score: 27.52\n",
      "Episode 212\tAverage Score: 25.80\tLast Score: 27.75\n",
      "Episode 213\tAverage Score: 25.89\tLast Score: 29.58\n",
      "Episode 214\tAverage Score: 25.99\tLast Score: 28.48\n",
      "Episode 215\tAverage Score: 26.08\tLast Score: 28.33\n",
      "Episode 216\tAverage Score: 26.15\tLast Score: 29.05\n",
      "Episode 217\tAverage Score: 26.24\tLast Score: 28.43\n",
      "Episode 218\tAverage Score: 26.33\tLast Score: 29.28\n",
      "Episode 219\tAverage Score: 26.43\tLast Score: 29.21\n",
      "Episode 220\tAverage Score: 26.51\tLast Score: 29.85\n",
      "Episode 220\tAverage Score: 26.51\tLast Score: 29.85\n",
      "Episode 221\tAverage Score: 26.61\tLast Score: 28.54\n",
      "Episode 222\tAverage Score: 26.71\tLast Score: 30.17\n",
      "Episode 223\tAverage Score: 26.80\tLast Score: 30.76\n",
      "Episode 224\tAverage Score: 26.87\tLast Score: 29.23\n",
      "Episode 225\tAverage Score: 26.95\tLast Score: 29.85\n",
      "Episode 226\tAverage Score: 27.01\tLast Score: 29.45\n",
      "Episode 227\tAverage Score: 27.10\tLast Score: 30.68\n",
      "Episode 228\tAverage Score: 27.16\tLast Score: 28.54\n",
      "Episode 229\tAverage Score: 27.24\tLast Score: 30.31\n",
      "Episode 230\tAverage Score: 27.29\tLast Score: 28.74\n",
      "Episode 230\tAverage Score: 27.29\tLast Score: 28.74\n",
      "Episode 231\tAverage Score: 27.34\tLast Score: 28.73\n",
      "Episode 232\tAverage Score: 27.40\tLast Score: 29.62\n",
      "Episode 233\tAverage Score: 27.46\tLast Score: 28.03\n",
      "Episode 234\tAverage Score: 27.51\tLast Score: 29.24\n",
      "Episode 235\tAverage Score: 27.55\tLast Score: 28.34\n",
      "Episode 236\tAverage Score: 27.61\tLast Score: 29.58\n",
      "Episode 237\tAverage Score: 27.67\tLast Score: 30.22\n",
      "Episode 238\tAverage Score: 27.73\tLast Score: 29.08\n",
      "Episode 239\tAverage Score: 27.79\tLast Score: 30.41\n",
      "Episode 240\tAverage Score: 27.85\tLast Score: 30.70\n",
      "Episode 240\tAverage Score: 27.85\tLast Score: 30.70\n",
      "Episode 241\tAverage Score: 27.90\tLast Score: 30.74\n",
      "Episode 242\tAverage Score: 27.96\tLast Score: 30.92\n",
      "Episode 243\tAverage Score: 28.00\tLast Score: 30.26\n",
      "Episode 244\tAverage Score: 28.04\tLast Score: 29.16\n",
      "Episode 245\tAverage Score: 28.09\tLast Score: 29.71\n",
      "Episode 246\tAverage Score: 28.14\tLast Score: 31.71\n",
      "Episode 247\tAverage Score: 28.19\tLast Score: 30.58\n",
      "Episode 248\tAverage Score: 28.24\tLast Score: 31.06\n",
      "Episode 249\tAverage Score: 28.28\tLast Score: 29.88\n",
      "Episode 250\tAverage Score: 28.33\tLast Score: 31.16\n",
      "Episode 250\tAverage Score: 28.33\tLast Score: 31.16\n",
      "Episode 251\tAverage Score: 28.38\tLast Score: 30.05\n",
      "Episode 252\tAverage Score: 28.41\tLast Score: 29.24\n",
      "Episode 253\tAverage Score: 28.45\tLast Score: 30.27\n",
      "Episode 254\tAverage Score: 28.48\tLast Score: 30.33\n",
      "Episode 255\tAverage Score: 28.52\tLast Score: 30.96\n",
      "Episode 256\tAverage Score: 28.57\tLast Score: 29.36\n",
      "Episode 257\tAverage Score: 28.60\tLast Score: 29.89\n",
      "Episode 258\tAverage Score: 28.66\tLast Score: 31.01\n",
      "Episode 259\tAverage Score: 28.69\tLast Score: 29.61\n",
      "Episode 260\tAverage Score: 28.72\tLast Score: 30.12\n",
      "Episode 260\tAverage Score: 28.72\tLast Score: 30.12\n",
      "Episode 261\tAverage Score: 28.75\tLast Score: 29.44\n",
      "Episode 262\tAverage Score: 28.77\tLast Score: 28.30\n",
      "Episode 263\tAverage Score: 28.78\tLast Score: 28.80\n",
      "Episode 264\tAverage Score: 28.84\tLast Score: 30.93\n",
      "Episode 265\tAverage Score: 28.86\tLast Score: 30.34\n",
      "Episode 266\tAverage Score: 28.88\tLast Score: 29.29\n",
      "Episode 267\tAverage Score: 28.89\tLast Score: 28.27\n",
      "Episode 268\tAverage Score: 28.90\tLast Score: 28.64\n",
      "Episode 269\tAverage Score: 28.92\tLast Score: 29.86\n",
      "Episode 270\tAverage Score: 28.92\tLast Score: 28.69\n",
      "Episode 270\tAverage Score: 28.92\tLast Score: 28.69\n",
      "Episode 271\tAverage Score: 28.96\tLast Score: 31.80\n",
      "Episode 272\tAverage Score: 28.99\tLast Score: 29.72\n",
      "Episode 273\tAverage Score: 29.00\tLast Score: 29.31\n",
      "Episode 274\tAverage Score: 29.03\tLast Score: 30.13\n",
      "Episode 275\tAverage Score: 29.04\tLast Score: 30.27\n",
      "Episode 276\tAverage Score: 29.05\tLast Score: 30.20\n",
      "Episode 277\tAverage Score: 29.07\tLast Score: 29.58\n",
      "Episode 278\tAverage Score: 29.10\tLast Score: 30.11\n",
      "Episode 279\tAverage Score: 29.11\tLast Score: 28.69\n",
      "Episode 280\tAverage Score: 29.14\tLast Score: 30.40\n",
      "Episode 280\tAverage Score: 29.14\tLast Score: 30.40\n",
      "Episode 281\tAverage Score: 29.18\tLast Score: 30.88\n",
      "Episode 282\tAverage Score: 29.19\tLast Score: 29.26\n",
      "Episode 283\tAverage Score: 29.21\tLast Score: 30.54\n",
      "Episode 284\tAverage Score: 29.24\tLast Score: 29.29\n",
      "Episode 285\tAverage Score: 29.31\tLast Score: 30.66\n",
      "Episode 286\tAverage Score: 29.32\tLast Score: 29.25\n",
      "Episode 287\tAverage Score: 29.37\tLast Score: 30.42\n",
      "Episode 288\tAverage Score: 29.40\tLast Score: 29.63\n",
      "Episode 289\tAverage Score: 29.44\tLast Score: 31.49\n",
      "Episode 290\tAverage Score: 29.47\tLast Score: 30.21\n",
      "Episode 290\tAverage Score: 29.47\tLast Score: 30.21\n",
      "Episode 291\tAverage Score: 29.50\tLast Score: 29.49\n",
      "Episode 292\tAverage Score: 29.54\tLast Score: 31.49\n",
      "Episode 293\tAverage Score: 29.57\tLast Score: 32.35\n",
      "Episode 294\tAverage Score: 29.61\tLast Score: 30.45\n",
      "Episode 295\tAverage Score: 29.62\tLast Score: 30.09\n",
      "Episode 296\tAverage Score: 29.65\tLast Score: 32.57\n",
      "Episode 297\tAverage Score: 29.68\tLast Score: 32.29\n",
      "Episode 298\tAverage Score: 29.71\tLast Score: 32.66\n",
      "Episode 299\tAverage Score: 29.77\tLast Score: 33.65\n",
      "Episode 300\tAverage Score: 29.82\tLast Score: 32.91\n",
      "Episode 300\tAverage Score: 29.82\tLast Score: 32.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 301\tAverage Score: 29.85\tLast Score: 32.31\n",
      "Episode 302\tAverage Score: 29.87\tLast Score: 31.49\n",
      "Episode 303\tAverage Score: 29.88\tLast Score: 30.54\n",
      "Episode 304\tAverage Score: 29.90\tLast Score: 29.26\n",
      "Episode 305\tAverage Score: 29.92\tLast Score: 30.48\n",
      "Episode 306\tAverage Score: 29.94\tLast Score: 29.02\n",
      "Episode 307\tAverage Score: 29.95\tLast Score: 30.08\n",
      "Episode 308\tAverage Score: 29.96\tLast Score: 29.61\n",
      "Episode 309\tAverage Score: 29.98\tLast Score: 29.62\n",
      "Episode 310\tAverage Score: 30.00\tLast Score: 31.58\n",
      "Episode 310\tAverage Score: 30.00\tLast Score: 31.58\n",
      "\n",
      "Environment solved in 210 episodes!\tAverage Score: 30.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size)\n",
    "ROLLOUT_LENGTH = 1024\n",
    "\n",
    "def A2C_run(n_episodes=500):\n",
    "    \"\"\"Advantage Actor Critic Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "    \"\"\"\n",
    "    all_scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)      # last 100 scores\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        roll_log_probs = []\n",
    "        roll_values = []\n",
    "        roll_rewards = []\n",
    "        roll_dones = []\n",
    "        roll_actions = []\n",
    "        roll_states = []\n",
    "        for _ in range(ROLLOUT_LENGTH):\n",
    "            actions, log_probs, values = agent.act(states, train=True)\n",
    "            roll_log_probs.append(log_probs.detach())\n",
    "            roll_values.append(values.detach())\n",
    "            roll_actions.append(actions.detach())\n",
    "            roll_states.append(states)\n",
    "            env_info = env.step(actions.cpu().detach().data.numpy())[brain_name]       # send all actions to the environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            roll_rewards.append(rewards)\n",
    "            roll_dones.append(dones)\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        actions, log_probs, values = agent.act(states, train=True)\n",
    "        roll_values.append(values.detach())\n",
    "        agent.step(roll_log_probs, roll_values, roll_rewards, roll_dones, roll_actions, roll_states)\n",
    "        scores_window.append(np.mean(scores))        # save most recent score\n",
    "        all_scores.append(np.mean(scores))           # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tLast Score: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(scores)))\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tLast Score: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(scores)))\n",
    "        if np.mean(scores_window)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.network.state_dict(), 'ppo_checkpoint.pth')\n",
    "            break\n",
    "    return all_scores\n",
    "\n",
    "scores = A2C_run()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 30.853499310370534\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "for _ in range(1024):\n",
    "    actions, log_probs, values = agent.act(states, train=False)\n",
    "    env_info = env.step(actions.cpu().detach().data.numpy())[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
